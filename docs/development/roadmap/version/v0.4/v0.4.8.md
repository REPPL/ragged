# v0.4.8 - Behaviour Learning Part 2: Personalised Ranking

**Hours**: 18-20 | **Priority**: P0 - Core Feature | **Status**: Planned

**Dependencies**: v0.4.7 complete (Interest profiles built)

---

## Overview

Implement personalised ranking algorithm that leverages interest profiles to improve retrieval relevance based on learned user behaviour.

**Vision**: Ragged automatically prioritises information aligned with user interests, improving relevance over time without manual configuration.

**Theoretical Foundation**: Extends [Context Engineering 2.0](../../../acknowledgements/context-engineering-2.0.md) **context-aware retrieval** by applying learned interest profiles to re-rank results. Personalisation reduces uncertainty (entropy reduction) by prioritising context aligned with user focus areas.

---

## Core Deliverables

### 1. Personalised Ranking Algorithm (10-12h)

Re-rank retrieval results using personal context from interest profiles.

#### Core Algorithm

```python
class PersonalisedRanker:
    """Re-rank results based on user interest profile."""

    def rerank(
        self,
        results: List[Document],
        query: str,
        persona: str,
        k: int
    ) -> List[Document]:
        """Re-rank results using personalisation.

        Args:
            results: Initial retrieval results (top-2k)
            query: User query
            persona: Current persona
            k: Number of final results

        Returns:
            Top-k personalised results
        """
        profile = self.profile_manager.get_profile(persona)
        query_topics = self.extractor.extract_topics(query)

        # Score each document
        scored_results = []
        for doc in results:
            base_score = doc.similarity_score  # From retrieval
            personalisation_score = self._calculate_personalisation(
                doc, query_topics, profile
            )
            final_score = self._combine_scores(
                base_score, personalisation_score
            )
            scored_results.append((doc, final_score))

        # Sort by final score and return top-k
        scored_results.sort(key=lambda x: x[1], reverse=True)
        return [doc for doc, _ in scored_results[:k]]

    def _calculate_personalisation(
        self,
        doc: Document,
        query_topics: List[Topic],
        profile: InterestProfile
    ) -> float:
        """Calculate personalisation score for document.

        Factors:
        1. Topic relevance: Doc topics match user interests
        2. Historical access: Doc previously accessed
        3. Recency: Recent interests weighted more
        4. Co-occurrence: Related to query topics
        """
        # Extract document topics
        doc_topics = self.extractor.extract_from_document(doc)

        # Topic relevance boost
        topic_boost = 0.0
        for doc_topic in doc_topics:
            if doc_topic.name in profile.topics:
                interest = profile.topics[doc_topic.name]
                # Weight by confidence and recency
                topic_boost += interest.confidence * interest.recency

        # Historical access boost
        history_boost = 0.0
        if doc.id in profile.get_accessed_documents():
            last_access = profile.get_last_access(doc.id)
            history_boost = self._time_decay(last_access)

        # Co-occurrence boost
        cooccurrence_boost = 0.0
        for query_topic in query_topics:
            if query_topic.name in profile.topics:
                interest = profile.topics[query_topic.name]
                for doc_topic in doc_topics:
                    if doc_topic.name in interest.co_occurring_topics:
                        cooccurrence_boost += 0.1

        return (
            topic_boost * 0.5 +
            history_boost * 0.3 +
            cooccurrence_boost * 0.2
        )

    def _combine_scores(
        self,
        base_score: float,
        personalisation_score: float,
        alpha: float = 0.3
    ) -> float:
        """Combine base retrieval score with personalisation.

        Args:
            base_score: Similarity score from retrieval (0-1)
            personalisation_score: Personalisation boost (0-1+)
            alpha: Personalisation weight (0-1)

        Returns:
            Combined score
        """
        return (1 - alpha) * base_score + alpha * personalisation_score
```

**Scoring Components**:

1. **Topic Relevance Boost** (50%):
   - Match document topics to user interests
   - Weight by interest confidence
   - Apply recency decay

2. **Historical Access Boost** (30%):
   - Previously accessed documents boosted
   - Time decay applied
   - Avoid over-promoting old documents

3. **Co-occurrence Boost** (20%):
   - Topics frequently seen together
   - Context-aware recommendations

**Configurable Parameters**:
```yaml
personalised_ranking:
  enabled: true
  alpha: 0.3  # Personalisation weight (0-1)
  retrieve_multiplier: 2  # Retrieve k*2, rerank to k
  min_confidence_threshold: 0.3
  time_decay_rate: 0.05  # Per day
```

**Files**:
- `ragged/memory/personalisation.py` (~350 lines)
- `ragged/memory/scoring.py` (~200 lines)
- `tests/memory/test_personalisation.py` (~250 lines)

---

### 2. RAG Pipeline Integration (5-6h)

Integrate personalised ranking into query flow.

#### Enhanced Pipeline

```python
# Before (v0.4.7)
query() ->
  switch_persona() ->
  enhance_with_context() ->
  retrieve(k) ->
  generate() ->
  record_interaction()

# After (v0.4.8)
query() ->
  switch_persona() ->
  enhance_with_context() ->
  retrieve(k * 2) ->                    # Retrieve more candidates
  personalised_rerank(k) ->             # NEW: Rerank using profile
  generate() ->
  record_interaction() ->
  update_profile_async()                # Background profile update
```

#### Implementation

**Modified**: `ragged/core/retrieval.py`
```python
class PersonalisedRetriever(Retriever):
    """Retriever with personalised ranking."""

    def __init__(self, base_retriever: Retriever, ranker: PersonalisedRanker):
        self.base_retriever = base_retriever
        self.ranker = ranker

    def retrieve(
        self,
        query: str,
        k: int,
        persona: str,
        personalise: bool = True
    ) -> List[Document]:
        """Retrieve with optional personalisation.

        Args:
            query: User query
            k: Number of results
            persona: Current persona
            personalise: Enable personalised ranking

        Returns:
            Top-k results (personalised if enabled)
        """
        # Retrieve k*2 candidates for reranking
        multiplier = 2 if personalise else 1
        candidates = self.base_retriever.retrieve(query, k * multiplier)

        if not personalise:
            return candidates[:k]

        # Personalised reranking
        return self.ranker.rerank(candidates, query, persona, k)
```

**Modified**: `ragged/main.py` - Update query command to use personalised retrieval

**Files**:
- Update `ragged/core/retrieval.py` (~200 lines added)
- Update `ragged/main.py` (~100 lines modified)
- `tests/core/test_personalised_retrieval.py` (~200 lines)

---

### 3. Interest Profile Analytics (2-3h)

Provide insights into interest profile effectiveness.

```bash
# View profile analytics
ragged memory analytics --persona researcher

# Compare personalised vs non-personalised
ragged memory compare-ranking --query "your query" --persona researcher
```

**Analytics Output**:
```bash
$ ragged memory analytics --persona researcher

Interest Profile Analytics: researcher

Profile Statistics:
- Topics tracked: 37
- High confidence topics (>0.7): 8
- Recent activity (last 7d): 24 queries
- Profile age: 45 days

Topic Distribution:
1. RAG (24 queries, 92% confidence)
2. privacy (18 queries, 87% confidence)
3. vector search (12 queries, 75% confidence)
...

Personalisation Impact:
- Avg relevance improvement: +15.3%
- Queries benefiting from personalisation: 78%
- Topics with strongest impact: RAG, privacy, NLP
```

**Files**:
- `ragged/memory/analytics.py` (~200 lines)
- Update CLI with analytics commands
- `tests/memory/test_analytics.py` (~150 lines)

---

### 4. A/B Testing Framework (3-4h)

Framework for evaluating personalisation effectiveness.

#### A/B Testing Infrastructure

```python
class ABTest:
    """A/B test personalised vs non-personalised ranking."""

    def run_test(
        self,
        queries: List[str],
        persona: str,
        duration_days: int = 7
    ) -> ABTestResults:
        """Run A/B test comparing ranking strategies.

        Args:
            queries: Test queries
            persona: Persona to test
            duration_days: Test duration

        Returns:
            Test results with statistical significance
        """
        results_a = []  # Non-personalised
        results_b = []  # Personalised

        for query in queries:
            # Variant A: Standard ranking
            docs_a = self.retriever.retrieve(
                query, k=10, persona=persona, personalise=False
            )
            results_a.append(self._evaluate_relevance(docs_a, query))

            # Variant B: Personalised ranking
            docs_b = self.retriever.retrieve(
                query, k=10, persona=persona, personalise=True
            )
            results_b.append(self._evaluate_relevance(docs_b, query))

        return self._calculate_significance(results_a, results_b)
```

**Metrics Tracked**:
- Precision@k
- NDCG (Normalized Discounted Cumulative Gain)
- User satisfaction (if feedback available)
- Latency difference

**CLI Commands**:
```bash
# Start A/B test
ragged memory ab-test start --queries test_queries.txt --persona researcher

# View A/B test results
ragged memory ab-test results --test-id <id>

# Compare variants
ragged memory ab-test compare --metric ndcg
```

**Files**:
- `ragged/memory/ab_testing.py` (~250 lines)
- `ragged/cli/commands/ab_test.py` (~150 lines)
- `tests/memory/test_ab_testing.py` (~150 lines)

---

### 5. Configuration & User Controls (1-2h)

User-facing controls for personalisation.

```bash
# Enable/disable personalisation
ragged memory personalisation enable --persona researcher
ragged memory personalisation disable --persona researcher

# Configure personalisation parameters
ragged memory config set personalisation.alpha 0.5
ragged memory config set personalisation.retrieve_multiplier 3

# View current configuration
ragged memory config show personalisation
```

**Configuration File** (`~/.ragged/config/memory.yaml`):
```yaml
personalised_ranking:
  enabled: true
  alpha: 0.3  # Personalisation weight (0 = no personalisation, 1 = full)
  retrieve_multiplier: 2  # Retrieve k*multiplier, rerank to k
  min_confidence_threshold: 0.3
  boost_factors:
    topic_relevance: 0.5
    historical_access: 0.3
    co_occurrence: 0.2
```

---

## Testing Requirements

**Coverage**: ≥85% for personalised ranking modules

**Test Categories**:

1. **Ranking Algorithm Tests**:
   - ✅ Topic relevance boost works
   - ✅ Historical access boost works
   - ✅ Co-occurrence boost works
   - ✅ Score combination reasonable
   - ✅ Edge cases: empty profile, no matches

2. **Pipeline Integration Tests**:
   - ✅ Personalised retrieval works end-to-end
   - ✅ Non-personalised mode still works
   - ✅ Profile updates after interaction
   - ✅ Performance acceptable

3. **Analytics Tests**:
   - ✅ Profile statistics accurate
   - ✅ Impact metrics calculated correctly
   - ✅ Comparison mode works

4. **A/B Testing Tests**:
   - ✅ Test runs correctly
   - ✅ Statistical significance calculated
   - ✅ Results persist and retrievable

**Test Files** (~900 lines):
- `tests/memory/test_personalisation.py` (~250 lines)
- `tests/core/test_personalised_retrieval.py` (~200 lines)
- `tests/memory/test_analytics.py` (~150 lines)
- `tests/memory/test_ab_testing.py` (~150 lines)
- `tests/integration/test_personalised_pipeline.py` (~150 lines)

---

## Documentation

**Required** (~1,200 lines):

1. **Tutorial**: Personalised Search with Ragged (~400 lines)
   - How personalisation works
   - Enabling/disabling personalisation
   - Viewing analytics
   - Tuning parameters

2. **Guide**: Personalised Ranking Deep Dive (~500 lines)
   - Algorithm explained
   - Scoring components
   - Configuration options
   - A/B testing guide
   - Best practices

3. **Reference**: Personalisation API (~300 lines)
   - PersonalisedRanker API
   - Analytics API
   - A/B Testing API
   - Configuration reference

---

## Success Criteria

Version 0.4.8 is successful if:

1. ✅ Personalisation improves relevance by >15% (measured by A/B testing)
2. ✅ Ranking algorithm produces intuitive results
3. ✅ Users can enable/disable personalisation easily
4. ✅ Analytics provide clear insights
5. ✅ A/B testing framework functional
6. ✅ Performance overhead acceptable (<2s end-to-end)
7. ✅ Pipeline integration seamless
8. ✅ 85%+ test coverage
9. ✅ Documentation complete
10. ✅ Configuration options clear and effective

---

## Performance Targets

- Personalised reranking: <100ms (for k*2 candidates)
- Profile retrieval: <50ms
- Analytics calculation: <500ms
- End-to-end query with personalisation: <2s
- A/B test execution: <5s for 10 queries

---

## File Summary

**New Files** (~2,500 lines):
- Personalised ranking core: ~1,000 lines
- Analytics & A/B testing: ~600 lines
- Tests: ~900 lines
- Documentation: ~1,200 lines
- CLI additions: ~150 lines

**Modified Files**:
- `ragged/core/retrieval.py` (~200 lines added)
- `ragged/main.py` (~100 lines modified)

**Dependencies**:
- No new external dependencies
- Builds on v0.4.7 interest profiles

---

## Privacy Considerations

**Data Used for Personalisation**:
- Interest profile (from v0.4.7)
- Query history
- Document access history

**Privacy Guarantees** (inherited from v0.4.5):
- All personalisation happens locally
- No external API calls
- User can disable personalisation entirely
- User can view all data used for ranking
- User can export ranking decisions

**Transparency**:
```bash
# Explain why document was ranked highly
ragged memory explain-ranking <doc-id> --query "your query"

# Output:
Document ranked #2 (score: 0.87)

Base similarity: 0.75
Personalisation boost: +0.12

Reasons:
- Topic "RAG" matches high-confidence interest (0.92)
- Previously accessed 3 days ago
- Related to recent queries about "retrieval"
```

---

## Configuration Examples

**Conservative Personalisation** (subtle influence):
```yaml
personalised_ranking:
  alpha: 0.2  # 20% personalisation, 80% base similarity
  min_confidence_threshold: 0.5  # Only high-confidence topics
```

**Aggressive Personalisation** (strong influence):
```yaml
personalised_ranking:
  alpha: 0.5  # 50/50 personalisation and base similarity
  min_confidence_threshold: 0.2  # Include more topics
  retrieve_multiplier: 3  # More candidates for reranking
```

**Disable Personalisation**:
```yaml
personalised_ranking:
  enabled: false
```

---

## Evaluation Metrics

**Relevance Improvement**:
- Target: >15% improvement over baseline
- Measured via A/B testing
- Metrics: Precision@k, NDCG, user feedback

**User Satisfaction**:
- Track user feedback on results
- Compare personalised vs non-personalised satisfaction

**Performance**:
- Latency overhead: <100ms for reranking
- End-to-end: <2s total

---

## Related Documentation

- [v0.4 Overview](README.md) - Release series overview
- [v0.4 Detailed Spec](v0.4-DETAILED-SPEC.md) - Part 2: Milestone 2
- [v0.4.7](v0.4.7.md) - Behaviour learning Part 1 (previous) - **Builds interest profiles**
- [v0.4.9](v0.4.9.md) - Refactoring (next)
- [v0.4.5 README](v0.4.5/README.md) - Memory foundation

---
