# v0.4.12 - Performance Optimisation & Benchmarking

**Hours**: 13-17 | **Priority**: P1 - Quality | **Status**: Planned

**Dependencies**: v0.4.11 complete (Backend migration tools)

---

## Overview

Comprehensive performance optimisation across the entire v0.4.x memory system stack, with emphasis on LEANN query optimisation, multi-backend benchmarking, and production-ready caching strategies.

**Vision**: Ensure ragged's memory system performs efficiently at scale (10K+ documents, complex temporal queries, personalised ranking) with minimal latency overhead.

**Rationale**: After building the complete memory system (v0.4.5-v0.4.10), this release focuses on making it production-ready through systematic performance optimisation and benchmarking infrastructure.

---

## Core Deliverables

### 1. LEANN Query Optimisation (4-5h)

Optimise LEANN backend for production workloads.

#### Graph Traversal Optimisation

**Current**: Naïve graph traversal for vector retrieval
**Target**: Optimised graph algorithms with early termination

```python
class OptimisedLEANNBackend:
    """LEANN backend with query optimisations."""

    def __init__(self, config: LEANNConfig):
        self.graph = self._initialise_graph(config)
        self.cache = LRUCache(maxsize=config.cache_size)
        self.query_planner = QueryPlanner()

    def query(
        self,
        query_vector: np.ndarray,
        k: int,
        filters: Optional[Dict] = None
    ) -> List[Document]:
        """Optimised query with caching and early termination."""

        # 1. Check cache
        cache_key = self._cache_key(query_vector, k, filters)
        if cache_key in self.cache:
            return self.cache[cache_key]

        # 2. Query planning
        plan = self.query_planner.plan(query_vector, k, filters, self.graph)

        # 3. Execute with optimisations
        if plan.use_approximate:
            results = self._approximate_search(query_vector, k, plan)
        else:
            results = self._exact_search(query_vector, k, plan)

        # 4. Cache results
        self.cache[cache_key] = results

        return results

    def _approximate_search(
        self,
        query_vector: np.ndarray,
        k: int,
        plan: QueryPlan
    ) -> List[Document]:
        """Approximate graph search with early termination.

        Optimisations:
        - Beam search instead of exhaustive traversal
        - Early termination when k good candidates found
        - Distance-based pruning
        """
        beam_width = min(k * 3, 100)  # Adaptive beam width
        candidates = []

        # Start from entry points (optimised selection)
        entry_points = self._select_entry_points(query_vector, plan)

        for entry in entry_points:
            # Beam search from entry point
            visited = set()
            current_beam = [(entry, self._distance(query_vector, entry))]

            while current_beam and len(candidates) < k * 2:
                # Expand beam
                next_beam = []
                for node, dist in current_beam[:beam_width]:
                    if node in visited:
                        continue
                    visited.add(node)

                    # Add to candidates
                    candidates.append((node, dist))

                    # Explore neighbours
                    for neighbour in self.graph.neighbours(node):
                        if neighbour not in visited:
                            neighbour_dist = self._distance(query_vector, neighbour)
                            next_beam.append((neighbour, neighbour_dist))

                # Sort and prune beam
                next_beam.sort(key=lambda x: x[1])
                current_beam = next_beam[:beam_width]

                # Early termination
                if self._early_termination_condition(candidates, k):
                    break

        # Return top-k
        candidates.sort(key=lambda x: x[1])
        return [node for node, _ in candidates[:k]]
```

#### Batch Query Optimisation

```python
def batch_query(
    self,
    query_vectors: List[np.ndarray],
    k: int
) -> List[List[Document]]:
    """Optimised batch querying.

    Optimisations:
    - Shared graph traversals
    - Vectorised distance calculations
    - Parallel processing (threading)
    """
    # Group similar queries for shared traversal
    query_groups = self._group_similar_queries(query_vectors)

    results = []
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = []
        for group in query_groups:
            future = executor.submit(
                self._batch_query_group, group, k
            )
            futures.append(future)

        for future in as_completed(futures):
            results.extend(future.result())

    return results
```

**Files**:
- `ragged/backend/leann_optimised.py` (~300 lines)
- `ragged/backend/query_planner.py` (~200 lines)
- `tests/backend/test_leann_optimisation.py` (~200 lines)

---

### 2. Memory System Performance Tuning (4-5h)

Optimise memory system components for production scale.

#### Database Query Optimisation

**SQLite Optimisations**:
```sql
-- Add indexes for common queries
CREATE INDEX idx_interactions_persona_timestamp
ON interactions(persona, timestamp DESC);

CREATE INDEX idx_interactions_session
ON interactions(session_id);

CREATE INDEX idx_temporal_facts_lookup
ON temporal_facts(persona, fact_type, valid_from, valid_to);

-- Enable WAL mode for better concurrency
PRAGMA journal_mode=WAL;

-- Increase cache size
PRAGMA cache_size=-64000;  -- 64MB cache

-- Use memory-mapped I/O for performance
PRAGMA mmap_size=268435456;  -- 256MB
```

**Kuzu Graph Optimisations**:
```python
# Optimise graph queries with prepared statements
class OptimisedGraphQueries:
    """Prepared graph queries for performance."""

    def __init__(self, graph_db):
        self.db = graph_db
        # Prepare common queries
        self.queries = {
            "user_topics": self.db.prepare(
                "MATCH (u:User {name: $user})-[r:INTERESTED_IN]->(t:Topic) "
                "RETURN t.name, r.frequency "
                "ORDER BY r.frequency DESC LIMIT $limit"
            ),
            "topic_cooccurrence": self.db.prepare(
                "MATCH (t1:Topic {name: $topic})<-[:INTERESTED_IN]-(u:User)-[:INTERESTED_IN]->(t2:Topic) "
                "WHERE t1 <> t2 "
                "RETURN t2.name, COUNT(*) as frequency "
                "ORDER BY frequency DESC LIMIT $limit"
            )
        }

    def get_user_topics(self, user: str, limit: int = 10):
        """Optimised user topics query."""
        return self.queries["user_topics"].execute(user=user, limit=limit)
```

#### Caching Strategy

```python
class MemorySystemCache:
    """Multi-level caching for memory system."""

    def __init__(self, config: CacheConfig):
        # L1: In-memory cache (hot data)
        self.l1_cache = LRUCache(maxsize=config.l1_size)

        # L2: Profile cache (user profiles)
        self.profile_cache = TTLCache(
            maxsize=config.l2_size,
            ttl=config.profile_ttl
        )

        # L3: Query result cache
        self.query_cache = TTLCache(
            maxsize=config.l3_size,
            ttl=config.query_ttl
        )

    def get_profile(self, persona: str) -> Optional[InterestProfile]:
        """Get profile with multi-level cache."""
        # Check L1
        if persona in self.l1_cache:
            return self.l1_cache[persona]

        # Check L2
        if persona in self.profile_cache:
            profile = self.profile_cache[persona]
            self.l1_cache[persona] = profile  # Promote to L1
            return profile

        return None  # Cache miss
```

**Files**:
- `ragged/memory/optimisation.py` (~250 lines)
- `ragged/memory/caching.py` (~300 lines)
- `tests/memory/test_optimisation.py` (~200 lines)

---

### 3. Multi-Backend Benchmarking Suite (3-4h)

Comprehensive benchmarking infrastructure for comparing backends and configurations.

#### Benchmark Framework

```python
class MemoryBenchmark:
    """Comprehensive memory system benchmarking."""

    def run_full_benchmark(
        self,
        backends: List[str],
        corpus_sizes: List[int],
        queries_per_test: int = 100
    ) -> BenchmarkReport:
        """Run complete benchmark suite.

        Tests:
        1. Ingest performance
        2. Query latency (single, batch)
        3. Memory usage
        4. Storage efficiency
        5. Concurrent access
        6. Personalised ranking overhead
        7. Temporal query performance
        """

        results = {}
        for backend in backends:
            results[backend] = {}
            for size in corpus_sizes:
                results[backend][size] = {
                    "ingest": self._benchmark_ingest(backend, size),
                    "query": self._benchmark_query(backend, queries_per_test),
                    "memory": self._benchmark_memory(backend, size),
                    "storage": self._measure_storage(backend, size),
                    "concurrent": self._benchmark_concurrent(backend),
                    "personalisation": self._benchmark_personalisation(backend),
                    "temporal": self._benchmark_temporal(backend)
                }

        return BenchmarkReport(results)
```

#### Benchmark Scenarios

**1. Ingest Performance**:
- Documents per second
- Memory usage during ingest
- Peak memory consumption

**2. Query Performance**:
- Single query latency (p50, p95, p99)
- Batch query throughput
- Cold vs warm cache performance

**3. Memory System Overhead**:
- Baseline query (no memory)
- With interest profiling
- With personalised ranking
- With temporal filtering

**4. Concurrent Access**:
- Multiple personas querying simultaneously
- Read/write contention
- Lock contention

**5. Scale Testing**:
- 1K, 10K, 50K, 100K documents
- Performance degradation curve
- Memory growth

#### CLI Benchmark Commands

```bash
# Full benchmark suite
ragged benchmark full \
  --backends chromadb,leann \
  --corpus-sizes 1000,10000,50000 \
  --output benchmark_report.html

# Quick benchmark
ragged benchmark quick --backend leann

# Specific benchmark
ragged benchmark query --backend chromadb --queries 1000

# Regression testing
ragged benchmark compare \
  --baseline v0.4.11 \
  --current v0.4.12 \
  --fail-on-regression 5%

# Continuous benchmarking (for CI/CD)
ragged benchmark ci \
  --corpus-size 1000 \
  --threshold-file benchmarks/thresholds.yaml
```

**Output Example**:
```bash
$ ragged benchmark full --backends chromadb,leann --corpus-sizes 1000,10000

Memory System Benchmark Report
================================

Corpus: 1,000 documents

                     ChromaDB    LEANN       Difference
-----------------------------------------------------------
Ingest Performance
  Docs/sec           245         198         -19.2%
  Peak memory        1.2 GB      0.8 GB      -33.3%

Query Performance (p95)
  Single query       45 ms       38 ms       -15.6% ✓
  Batch (10)         180 ms      145 ms      -19.4% ✓
  w/ Personalisation 125 ms      95 ms       -24.0% ✓

Storage
  Total size         197 MB      18 MB       -90.9% ✓
  Per document       202 KB      18 KB       -91.1% ✓

Memory System Overhead
  Baseline           42 ms       35 ms
  + Profiling        +8 ms       +6 ms
  + Personalisation  +35 ms      +25 ms
  + Temporal         +12 ms      +10 ms

Overall Winner: LEANN (query speed + storage efficiency)
```

**Files**:
- `ragged/benchmark/framework.py` (~400 lines)
- `ragged/benchmark/scenarios.py` (~300 lines)
- `ragged/cli/commands/benchmark.py` (~200 lines)
- `tests/benchmark/test_framework.py` (~200 lines)

---

### 4. Profiling & Monitoring Tools (2-3h)

Runtime profiling and monitoring for production deployments.

#### Performance Profiling

```python
class PerformanceProfiler:
    """Profile ragged performance in production."""

    def enable(self, components: List[str] = None):
        """Enable profiling for components.

        Args:
            components: ["query", "memory", "backend", "all"]
        """
        self.enabled = True
        self.components = components or ["all"]
        self.metrics = defaultdict(list)

    def profile_query(self, query: str, persona: str):
        """Profile query execution."""
        with self.timer("query_total"):
            # Profile each stage
            with self.timer("query.enhance"):
                enhanced = enhance_with_context(query, persona)

            with self.timer("query.retrieve"):
                results = retrieve(enhanced, k=20)

            with self.timer("query.rerank"):
                reranked = personalised_rerank(results, persona, k=10)

            with self.timer("query.generate"):
                response = generate(query, reranked)

        return response

    def get_report(self) -> PerformanceReport:
        """Get performance report."""
        return PerformanceReport({
            component: {
                "count": len(timings),
                "mean": np.mean(timings),
                "p50": np.percentile(timings, 50),
                "p95": np.percentile(timings, 95),
                "p99": np.percentile(timings, 99),
            }
            for component, timings in self.metrics.items()
        })
```

#### CLI Profiling

```bash
# Enable profiling
ragged profile enable --components query,memory

# Run queries with profiling
ragged query "your query" --profile

# Get profiling report
ragged profile report --since "-1h"

# Output:
Performance Report (last 1 hour)
=================================

Query Performance (42 queries):
  Mean: 1.23s | p50: 1.15s | p95: 2.34s | p99: 3.12s

Breakdown:
  enhance:        125ms (10.2%)
  retrieve:       450ms (36.6%)
  rerank:         315ms (25.6%)
  generate:       340ms (27.6%)

Slowest queries:
  1. "complex temporal query..." - 3.12s
  2. "large result set query..." - 2.87s
  3. "multi-topic query..." - 2.45s

Recommendations:
  - Enable query cache (30% of queries repeat)
  - Increase LEANN beam width for complex queries
  - Consider result set size limits
```

**Files**:
- `ragged/monitoring/profiler.py` (~250 lines)
- `ragged/monitoring/metrics.py` (~150 lines)
- `ragged/cli/commands/profile.py` (~150 lines)

---

### 5. Configuration Tuning Guide (1-2h)

Documentation and tools for tuning ragged for different workloads.

#### Performance Profiles

**High Performance** (low latency, high memory):
```yaml
# ~/.ragged/config/performance.yaml
backend:
  type: leann
  cache_size: 10000
  beam_width: 150

memory:
  cache:
    l1_size: 1000
    l2_size: 500
    profile_ttl: 3600
    query_ttl: 1800

personalisation:
  retrieve_multiplier: 2
  async_updates: true

database:
  sqlite_cache_mb: 128
  mmap_size_mb: 512
```

**Balanced** (default):
```yaml
backend:
  type: leann
  cache_size: 5000
  beam_width: 100

memory:
  cache:
    l1_size: 500
    l2_size: 200
    profile_ttl: 1800
    query_ttl: 900
```

**Low Memory** (minimal footprint):
```yaml
backend:
  type: leann
  cache_size: 1000
  beam_width: 50

memory:
  cache:
    l1_size: 100
    l2_size: 50
    profile_ttl: 600
    query_ttl: 300

database:
  sqlite_cache_mb: 32
  mmap_size_mb: 128
```

#### Auto-Tuning

```bash
# Auto-tune based on workload
ragged tune auto --workload interactive  # Low latency
ragged tune auto --workload batch        # High throughput
ragged tune auto --workload memory       # Low memory

# Apply performance profile
ragged tune apply high-performance
ragged tune apply low-memory

# Custom tuning
ragged tune set backend.cache_size 10000
ragged tune set memory.cache.l1_size 1000
```

---

## Testing Requirements

**Coverage**: ≥80% for optimisation modules

**Performance Tests**:
- ✅ LEANN query optimisation improves latency
- ✅ Batch queries faster than individual queries
- ✅ Caching reduces repeated query time
- ✅ Database indexes improve query speed
- ✅ Memory overhead acceptable (<20%)
- ✅ No memory leaks in long-running tests
- ✅ Concurrent access safe and performant

**Benchmark Tests**:
- ✅ Benchmark framework produces consistent results
- ✅ Regression detection works
- ✅ All benchmark scenarios run successfully

**Test Files** (~700 lines):
- `tests/backend/test_leann_optimisation.py` (~200 lines)
- `tests/memory/test_optimisation.py` (~200 lines)
- `tests/benchmark/test_framework.py` (~200 lines)
- `tests/performance/test_regression.py` (~100 lines)

---

## Documentation

**Required** (~800 lines):

1. **Guide**: Performance Tuning Guide (~400 lines)
   - Understanding bottlenecks
   - Tuning for workloads
   - Performance profiles
   - Monitoring in production

2. **Reference**: Benchmark Reference (~200 lines)
   - Benchmark scenarios
   - Interpreting results
   - Regression testing
   - CI/CD integration

3. **Tutorial**: Optimising Ragged (~200 lines)
   - Profiling queries
   - Applying optimisations
   - Measuring improvements

---

## Success Criteria

Version 0.4.12 is successful if:

1. ✅ LEANN query latency improved by >20% vs v0.4.11
2. ✅ Memory system overhead <20% (vs baseline retrieval)
3. ✅ Caching reduces repeated queries by >50%
4. ✅ Batch queries >3x faster than individual queries
5. ✅ Concurrent access scales to 10+ personas
6. ✅ No memory leaks in 24-hour stability test
7. ✅ Benchmark suite comprehensive and reliable
8. ✅ Profiling tools functional and helpful
9. ✅ Performance profiles provide clear improvements
10. ✅ 80%+ test coverage
11. ✅ Documentation complete
12. ✅ Zero performance regressions from v0.4.11

---

## Performance Targets

**LEANN Backend**:
- Single query (1K docs): <40ms (p95)
- Single query (10K docs): <100ms (p95)
- Batch query (10 queries): <150ms (p95)

**Memory System**:
- Profile retrieval: <50ms
- Interest calculation: <100ms
- Personalised ranking: <100ms (for 20 candidates)
- Temporal query: <500ms

**End-to-End**:
- Query with personalisation (1K docs): <1.5s
- Query with personalisation (10K docs): <2.5s
- Query with temporal context: <2s

**Caching**:
- Cache hit: <10ms
- Cache miss penalty: <5ms

**Memory Usage**:
- Idle: <200MB
- Active (10K docs): <500MB
- Peak (50K docs): <1.5GB

---

## File Summary

**New Files** (~2,700 lines):
- Optimisation core: ~1,050 lines
- Benchmarking: ~900 lines
- Profiling & monitoring: ~550 lines
- Tests: ~700 lines
- Documentation: ~800 lines

**Modified Files**:
- `ragged/backend/leann.py` - Apply optimisations
- `ragged/memory/profile.py` - Add caching
- Database schemas - Add indexes

**Dependencies**:
- No new external dependencies
- Uses existing infrastructure

---

## Optimisation Techniques Summary

**Query Optimisation**:
- Beam search for graph traversal
- Early termination
- Query planning
- Vectorised operations

**Caching**:
- Multi-level cache (L1/L2/L3)
- LRU and TTL strategies
- Profile caching
- Query result caching

**Database Optimisation**:
- Prepared statements
- Strategic indexes
- WAL mode for SQLite
- Memory-mapped I/O

**Concurrency**:
- Connection pooling
- Read/write locks optimised
- Parallel batch processing

**Memory Management**:
- Lazy loading
- Streaming for large datasets
- Resource cleanup
- Bounded caches

---

## Regression Testing in CI/CD

**GitHub Actions Integration**:
```yaml
# .github/workflows/benchmarks.yml
name: Performance Benchmarks

on: [push, pull_request]

jobs:
  benchmark:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Run benchmarks
        run: |
          ragged benchmark ci \
            --corpus-size 1000 \
            --threshold-file benchmarks/thresholds.yaml

      - name: Check regressions
        run: |
          ragged benchmark compare \
            --baseline ${{ github.base_ref }} \
            --current ${{ github.sha }} \
            --fail-on-regression 10%
```

**Threshold File** (`benchmarks/thresholds.yaml`):
```yaml
query_latency_p95_ms: 100
ingest_docs_per_sec: 150
memory_usage_mb: 600
storage_per_doc_kb: 25
personalisation_overhead_ms: 50
```

---

## Related Documentation

- [v0.4 Overview](README.md) - Release series overview
- [v0.4.11](v0.4.11.md) - Backend migration (previous)
- [v0.4.13](v0.4.13.md) - Production readiness (next)
- [v0.4.3/README.md](v0.4.3/README.md) - LEANN backend
- [v0.4.5/README.md](v0.4.5/README.md) - Memory foundation

---
