# v0.5.5: Testing - Integration & E2E Validation

**Focus:** Quality assurance for all v0.5.x features

**Hours:** 12-16 hours

**Dependencies:** v0.5.0 through v0.5.4 (all features implemented)

**Status:** Planned

---

## Overview

Version 0.5.5 implements comprehensive integration and end-to-end testing to validate all v0.5.x features working together. This version ensures quality, catches regressions, and validates performance targets before documentation and security hardening.

**What This Version Delivers:**
- Integration test suite for all 6 VISION features
- End-to-end workflow tests (ingest → query → retrieve)
- Performance benchmarks validating targets
- Cross-platform tests (CUDA, MPS, CPU)
- CI/CD pipeline configuration

**Why This Version is Critical:**
- Validates features work together, not just in isolation
- Catches integration bugs before user-facing documentation
- Establishes performance baselines
- Ensures cross-platform compatibility

**Scope:** Integration and E2E tests only. Unit tests are created during feature implementation (v0.5.0-v0.5.4).

---

## Testing Strategy

### Integration Tests (8-10 hours)

**Session 1: Feature Integration Tests (4-5h)**

Tests all 6 VISION features working together:

1. **ColPali + Dual Storage Integration**
   - Vision embeddings stored correctly in dual schema
   - Text and vision embeddings coexist
   - Migration from v0.4 preserves text, adds vision

2. **GPU Management Integration**
   - Device detection works with ColPali embedder
   - OOM recovery triggers during batch embedding
   - Adaptive batch sizing improves throughput

3. **Vision Retrieval Integration**
   - Text/image/hybrid queries return correct results
   - Visual boosting changes result rankings
   - RRF score fusion works correctly

4. **CLI Integration**
   - All CLI commands execute successfully
   - Progress indicators update correctly
   - Error messages clear and actionable

5. **Web UI Integration**
   - Gradio app launches and loads
   - Upload → ingestion → query workflow works
   - GPU monitoring displays real-time data

6. **Cross-Feature Workflows**
   - Ingest with vision → query with hybrid → results correct
   - GPU monitoring during batch ingestion → stats accurate
   - CLI and UI operate on same storage (no conflicts)

**Test Coverage:**
```python
# tests/integration/test_vision_features.py (~400 lines)

def test_full_vision_pipeline():
    """Test complete vision pipeline."""
    # 1. Ingest PDF with vision
    doc_id = ingest_pdf_with_vision("test.pdf")

    # 2. Verify dual storage
    assert_text_embeddings_stored(doc_id)
    assert_vision_embeddings_stored(doc_id)

    # 3. Query with text
    text_results = query_text("architecture")
    assert len(text_results) > 0

    # 4. Query with image
    image_results = query_image("diagram.png")
    assert len(image_results) > 0

    # 5. Hybrid query
    hybrid_results = query_hybrid("architecture", "diagram.png")
    assert len(hybrid_results) > 0

def test_cross_platform_compatibility():
    """Test on CUDA, MPS, and CPU."""
    for device in ["cuda", "mps", "cpu"]:
        if not is_device_available(device):
            pytest.skip(f"{device} not available")

        # Run full pipeline on each device
        embedder = ColPaliEmbedder(device=device)
        embeddings = embedder.embed_page(test_image)
        assert embeddings.shape == (768,)
```

**Time:** 4-5 hours

---

**Session 2: Performance Benchmarks (4-5h)**

Validates all performance targets from v0.5.x:

**Embedding Performance:**
- [ ] Vision embedding: <5s/page (GPU), <15s/page (CPU)
- [ ] Batch processing: 30%+ speedup vs sequential
- [ ] Adaptive batch sizing: Targets 75% GPU utilisation

**Query Performance:**
- [ ] Text query: <200ms latency
- [ ] Image query: <500ms latency
- [ ] Hybrid query: <600ms latency
- [ ] Reranking: <50ms overhead for 100 results

**System Performance:**
- [ ] GPU memory: <8GB for standard workloads
- [ ] Storage operations: <100ms per embedding
- [ ] Migration: <1 minute per 1000 embeddings

**Benchmark Suite:**
```python
# tests/performance/test_benchmarks.py (~200 lines)

@pytest.mark.benchmark
def test_vision_embedding_latency(benchmark):
    """Benchmark vision embedding generation."""
    embedder = ColPaliEmbedder(device="cuda")
    image = load_test_image()

    result = benchmark(embedder.embed_page, image)

    # Assert <5s on GPU
    assert result.stats.mean < 5.0

@pytest.mark.benchmark
def test_query_latency(benchmark):
    """Benchmark multi-modal query latency."""
    retriever = VisionRetriever()

    # Text query benchmark
    text_result = benchmark(retriever.query, text="test query")
    assert text_result.execution_time_ms < 200

    # Image query benchmark
    image_result = benchmark(retriever.query, image=test_image)
    assert image_result.execution_time_ms < 500
```

**Time:** 4-5 hours

---

### End-to-End Tests (4-6 hours)

**Session 3: E2E Workflow Tests (4-6h)**

Complete user workflows from start to finish:

**Workflow 1: New User Setup**
1. Install ragged
2. Ingest first document with vision
3. Perform text query
4. Perform image query
5. View results in Gradio UI

**Workflow 2: Power User Automation**
1. Batch ingest directory with CLI
2. Monitor GPU during ingestion
3. Run hybrid queries via CLI
4. Export results to JSON

**Workflow 3: Migration from v0.4**
1. Detect v0.4 collection
2. Run migration (dry-run first)
3. Verify all text embeddings preserved
4. Add vision embeddings to migrated docs

**Workflow 4: Cross-Platform Usage**
1. Same user runs on laptop (CPU)
2. Then on workstation (CUDA)
3. Same collection works on both
4. Performance degrades gracefully on CPU

**E2E Test Suite:**
```python
# tests/e2e/test_workflows.py (~300 lines)

def test_new_user_workflow():
    """Test complete new user experience."""
    # 1. Initialize ragged
    init_ragged()

    # 2. Ingest document
    doc_id = cli_ingest_pdf("sample.pdf", vision=True)
    assert doc_id is not None

    # 3. Query
    results = cli_query_text("test query")
    assert len(results) > 0

    # 4. UI access
    ui_url = launch_gradio_ui()
    response = requests.get(ui_url)
    assert response.status_code == 200
```

**Time:** 4-6 hours

---

## Implementation Checklist

- [ ] **Integration Tests** (8-10h)
  - [ ] Feature integration suite (400 lines)
  - [ ] Performance benchmarks (200 lines)
  - [ ] Cross-platform validation

- [ ] **E2E Tests** (4-6h)
  - [ ] User workflow tests (300 lines)
  - [ ] CLI automation tests
  - [ ] Cross-platform tests

---

## Success Criteria

### Test Coverage

- [ ] All 6 VISION features have integration tests
- [ ] All major workflows have E2E tests
- [ ] Cross-platform tests pass on CUDA/MPS/CPU
- [ ] Performance benchmarks meet targets

### Quality Gates

- [ ] All tests pass (100% pass rate)
- [ ] No flaky tests (3 consecutive runs pass)
- [ ] Performance targets met (see targets above)
- [ ] CI/CD pipeline green

### Documentation

- [ ] All tests documented with docstrings
- [ ] Test fixtures documented
- [ ] Known issues tracked in GitHub Issues

---

## Files Created

**New Files (~900 lines):**
- `tests/integration/test_vision_features.py` (~400 lines)
- `tests/performance/test_benchmarks.py` (~200 lines)
- `tests/e2e/test_workflows.py` (~300 lines)
- `.github/workflows/integration-tests.yml` (~50 lines) - CI/CD configuration

---

## CI/CD Pipeline

**GitHub Actions Workflow:**
```yaml
name: Integration Tests

on: [push, pull_request]

jobs:
  integration-tests:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: pip install -e ".[dev]"

      - name: Run integration tests
        run: pytest tests/integration/ -v

      - name: Run E2E tests
        run: pytest tests/e2e/ -v

      - name: Run benchmarks (CPU only in CI)
        run: pytest tests/performance/ -m benchmark
```

---

## Related Documentation

**Context:**
- [v0.5.x Overview](./README.md)
- [Testing Specification](./testing.md) - Comprehensive test coverage details

**Dependencies:**
- All of v0.5.0 through v0.5.4

**Next Steps:**
- [v0.5.6: Documentation](./v0.5.6.md) - Manual tests documented here
- [v0.5.7: Security](./v0.5.7.md) - Security testing builds on v0.5.5

---

**Status:** Planned - Quality assurance for v0.5.x
