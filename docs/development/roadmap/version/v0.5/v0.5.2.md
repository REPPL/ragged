# v0.5.2: Retrieval - Multi-Modal Vision Queries

**Focus:** Enable text, image, and hybrid query capabilities

**Hours:** 28-38 hours

**Dependencies:** v0.5.0 (ColPali + Dual Storage)

**Status:** Planned

---

## Overview

Version 0.5.2 transforms ragged's retrieval system to leverage vision embeddings for multi-modal queries. Users can now search documents using text queries, image queries (e.g., finding similar diagrams), or hybrid queries combining both modalities.

**What This Version Delivers:**
- Multi-modal query processing (text, image, or hybrid)
- Vision-aware retriever with RRF score fusion
- Visual content boosting (prioritise results with diagrams/tables)
- Visual reranking based on layout metadata
- CLI commands: `query text`, `query image`, `query hybrid`
- Query expansion with visual-relevant keywords

**Why This Version is Critical:**
- Unlocks vision capabilities from v0.5.0 for end users
- Enables use cases like "find the architecture diagram similar to this sketch"
- Provides intelligent result ranking combining text and visual similarity
- Makes vision features accessible via command-line interface

**Builds On:** v0.5.0's ColPaliEmbedder and DualEmbeddingStore infrastructure.

---

## Feature: VISION-003 Vision-Aware Retrieval (28-38 hours)

### Architecture Overview

**Retrieval Pipeline:**
```
User Query
    ↓
[Query Processor]
    ├─ Text Query → TextEmbedder → 384-dim embedding
    ├─ Image Query → ColPaliEmbedder → 768-dim embedding
    └─ Hybrid → Both embeddings
    ↓
[DualEmbeddingStore]
    ├─ Text-only search → Text embeddings
    ├─ Vision-only search → Vision embeddings
    └─ Hybrid search → Both, merged with RRF
    ↓
[Visual Reranker] (optional)
    ├─ Boost results with diagrams/tables
    ├─ Filter by layout complexity
    └─ Cross-modal relevance scoring
    ↓
Results (ranked by combined score)
```

**Query Types:**
1. **Text-only:** Traditional semantic search (existing capability enhanced)
2. **Image-only:** Upload image to find visually similar pages
3. **Hybrid:** Combined text + image search with configurable weights

---

## Implementation Plan

### Phase 1: Multi-Modal Query Processing (8-10 hours)

#### Session 1.1: Query Type Detection (3-4h)

**Purpose:** Implement query classification and embedding generation.

**Key Components:**
- `QueryType` enum (TEXT_ONLY, IMAGE_ONLY, HYBRID)
- `QueryEmbeddings` dataclass (container for query embeddings)
- `MultiModalQueryProcessor` class (query processing logic)

**Implementation Tasks:**
1. Detect query type from inputs (text, image, or both)
2. Generate text embeddings using TextEmbedder (384-dim)
3. Generate vision embeddings using ColPaliEmbedder (768-dim)
4. Validate embedding presence based on query type
5. Handle image input (file path or PIL Image)

**Deliverables:**
- `src/retrieval/query_processor.py` (~200 lines)
- Type-safe query processing
- Support for all three query modes

**Success Criteria:**
- [ ] Text queries generate 384-dim embeddings
- [ ] Image queries generate 768-dim embeddings
- [ ] Hybrid queries generate both embeddings
- [ ] Invalid queries raise ValueError
- [ ] Image paths and PIL Images both supported

**Time:** 3-4 hours

---

#### Session 1.2: Query Expansion for Vision (5-6h)

**Purpose:** Implement visual query expansion and augmentation.

**Key Components:**
- `VisualQueryExpander` class (query expansion logic)

**Expansion Strategies:**
1. **Text Expansion:** Add visual-relevant keywords
   - "architecture" → "architecture diagram", "architecture illustration"
   - Detection of visual intent keywords
2. **Image Augmentation:** Create query variations (future enhancement)
   - Rotations, crops, brightness adjustments
   - Disabled by default (computational cost)

**Implementation Tasks:**
1. Detect visual intent in text queries
2. Expand queries with visual keywords ("diagram", "chart", "figure")
3. Framework for image augmentation (basic implementation)
4. Configurable enable/disable for each strategy

**Deliverables:**
- `src/retrieval/query_expansion.py` (~150 lines)
- Text query expansion with visual keywords
- Image augmentation framework (optional)

**Success Criteria:**
- [ ] Queries with visual keywords expanded automatically
- [ ] Expansion generates 1-4 query variants
- [ ] Non-visual queries remain unchanged
- [ ] Image augmentation framework in place (disabled)

**Time:** 5-6 hours

---

### Phase 2: Vision-Aware Retriever (10-14 hours)

#### Session 2.1: VisionRetriever Class (6-8h)

**Purpose:** Implement multi-modal retrieval engine with unified interface.

**Key Components:**
- `RetrievalResult` dataclass (single result container)
- `RetrievalResponse` dataclass (complete response)
- `VisionRetriever` class (multi-modal retrieval engine)

**Query Methods:**
- `query(text, image, ...)` - Unified interface for all modes
- `_query_text_only()` - Text-only execution
- `_query_image_only()` - Image-only execution
- `_query_hybrid()` - Hybrid text+vision execution

**Visual Boosting:**
- `boost_diagrams` flag - Increase scores for results with diagrams (20% boost)
- `boost_tables` flag - Increase scores for results with tables (15% boost)
- Metadata-driven boosting (uses `has_diagrams`, `has_tables` fields)

**Implementation Tasks:**
1. Implement VisionRetriever with unified query interface
2. Route queries to appropriate execution method
3. Format ChromaDB results to RetrievalResult objects
4. Implement visual content boosting logic
5. Calculate execution time metrics

**Deliverables:**
- `src/retrieval/vision_retriever.py` (~400 lines)
- Unified interface for all query types
- Visual content boosting
- Type-safe result formatting

**Success Criteria:**
- [ ] Text queries return text embeddings results
- [ ] Image queries return vision embeddings results
- [ ] Hybrid queries merge both using RRF
- [ ] Visual boosting increases relevant result scores
- [ ] Execution time tracked and returned

**Time:** 6-8 hours

---

#### Session 2.2: Reranking with Visual Signals (4-6h)

**Purpose:** Implement vision-aware reranking strategies.

**Key Components:**
- `RerankerConfig` dataclass (reranking parameters)
- `VisualReranker` class (reranking logic)

**Reranking Strategies:**
1. **Visual Content Boosting:** Prefer results with diagrams/tables
2. **Layout Complexity Filtering:** Filter overly complex layouts
3. **Cross-Modal Alignment:** (future enhancement, placeholder)

**Configuration:**
- `diagram_weight` - Weight added for diagrams (default: 0.3)
- `table_weight` - Weight added for tables (default: 0.2)
- `layout_complexity_weight` - Weight for layout simplicity (default: 0.1)

**Implementation Tasks:**
1. Implement RerankerConfig with default weights
2. Implement visual content boosting (add to scores)
3. Implement layout complexity filtering
4. Re-sort results by adjusted scores
5. Update result ranks after reranking

**Deliverables:**
- `src/retrieval/reranker.py` (~150 lines)
- Visual content-aware reranking
- Layout complexity filtering
- Configurable reranking weights

**Success Criteria:**
- [ ] Reranker boosts diagram/table results correctly
- [ ] Layout complexity filtering works
- [ ] Results re-sorted and re-ranked
- [ ] Configurable weights applied

**Time:** 4-6 hours

---

### Phase 3: Integration & Configuration (6-8 hours)

#### Session 3.1: CLI Integration (3-4h)

**Purpose:** Add vision retrieval commands to CLI.

**New CLI Commands:**

**`ragged query text <TEXT>`**
- Execute text-only query
- Options: `--num-results`, `--boost-diagrams`, `--boost-tables`

**`ragged query image <IMAGE_PATH>`**
- Execute image-only query
- Options: `--num-results`

**`ragged query hybrid <TEXT> <IMAGE_PATH>`**
- Execute hybrid text+image query
- Options: `--num-results`, `--text-weight`, `--vision-weight`

**Output Format:**
```
Found 10 results in 245.32ms

1. doc_abc123 (score: 0.8542)
   Type: vision
   Page: 5

2. doc_xyz789 (score: 0.7891)
   Type: text
   Chunk: 12
```

**Implementation Tasks:**
1. Create `query` command group
2. Implement `query text` command
3. Implement `query image` command
4. Implement `query hybrid` command
5. Format and display results

**Deliverables:**
- Updated `src/cli.py` (+100 lines)
- Three query commands functional
- Result formatting and display

**Success Criteria:**
- [ ] All three query commands work from CLI
- [ ] Results formatted clearly
- [ ] Execution time displayed
- [ ] Help text comprehensive

**Time:** 3-4 hours

---

#### Session 3.2: Configuration Updates (3-4h)

**Purpose:** Add vision retrieval settings to configuration.

**New Configuration:**
```python
class RetrievalConfig(BaseSettings):
    # Multi-modal retrieval
    enable_vision_retrieval: bool = False
    default_text_weight: float = 0.5
    default_vision_weight: float = 0.5

    # Query expansion
    enable_query_expansion: bool = True
    enable_image_augmentation: bool = False

    # Visual boosting
    diagram_boost_factor: float = 1.2
    table_boost_factor: float = 1.15

    # Reranking
    enable_visual_reranking: bool = True
    diagram_rerank_weight: float = 0.3
    table_rerank_weight: float = 0.2
```

**Environment Variables:**
- `RAGGED_VISION_RETRIEVAL_ENABLED`
- `RAGGED_DEFAULT_TEXT_WEIGHT`
- `RAGGED_DEFAULT_VISION_WEIGHT`

**Implementation Tasks:**
1. Add RetrievalConfig to settings
2. Define default values for all parameters
3. Add environment variable mapping
4. Integrate config with VisionRetriever
5. Update documentation

**Deliverables:**
- Updated `src/config/settings.py` (+30 lines)
- Configuration schema for vision retrieval
- Environment variable support

**Success Criteria:**
- [ ] Configuration loaded from environment
- [ ] Default values sensible
- [ ] VisionRetriever respects config
- [ ] Settings validated on load

**Time:** 3-4 hours

---

### Phase 4: Testing (4-6 hours)

#### Session 4.1: Unit Tests (2-3h)

**Test Coverage:**

**Query Processing Tests:**
- Query type detection (text, image, hybrid)
- Text embedding generation
- Image embedding generation
- Invalid query handling

**Retrieval Tests:**
- Text-only query execution
- Image-only query execution (GPU required)
- Hybrid query execution
- Visual boosting logic
- Result formatting

**Reranking Tests:**
- Visual content boosting
- Layout complexity filtering
- Score adjustment logic

**Example Test:**
```python
# tests/retrieval/test_vision_retriever.py

def test_text_query(mock_retriever):
    """Test text-only query execution."""
    response = mock_retriever.query(
        text="database schema",
        n_results=5
    )

    assert response.query_type == QueryType.TEXT_ONLY
    assert len(response.results) <= 5
    assert all(r.embedding_type == "text" for r in response.results)

def test_visual_boosting():
    """Test diagram boosting increases scores."""
    response_no_boost = retriever.query(
        text="architecture", boost_diagrams=False
    )
    response_with_boost = retriever.query(
        text="architecture", boost_diagrams=True
    )

    # Results with diagrams should rank higher
    boosted_top = response_with_boost.results[0]
    assert boosted_top.metadata.get("has_diagrams") == True
```

**Deliverables:**
- `tests/retrieval/test_query_processor.py` (~150 lines)
- `tests/retrieval/test_vision_retriever.py` (~200 lines)
- `tests/retrieval/test_reranker.py` (~100 lines)

**Time:** 2-3 hours

---

#### Session 4.2: Integration Tests (2-3h)

**Test Coverage:**
- End-to-end query processing pipeline
- Hybrid retrieval with score fusion
- CLI command execution
- Performance benchmarks (latency targets)

**Integration Scenarios:**
1. Ingest document → Text query → Verify results
2. Ingest document with vision → Image query → Verify results
3. Hybrid query → Verify text and vision results merged
4. CLI commands → Verify output format

**Performance Benchmarks:**
- Text query: <200ms target
- Image query: <500ms target
- Hybrid query: <600ms target

**Deliverables:**
- `tests/integration/test_vision_retrieval.py` (~150 lines)
- Performance benchmark suite
- CLI integration tests

**Time:** 2-3 hours

---

## Implementation Checklist

### Session-by-Session Breakdown

**Phase 1: Multi-Modal Query Processing (8-10 hours)**

- [ ] **Session 1.1** (3-4h): Query Type Detection
  - Implement QueryType enum, QueryEmbeddings dataclass
  - Implement MultiModalQueryProcessor class
  - Query type detection logic
  - Text and image embedding generation
  - Test query processing

- [ ] **Session 1.2** (5-6h): Query Expansion
  - Implement VisualQueryExpander class
  - Text query expansion with visual keywords
  - Image augmentation framework (basic)
  - Test expansion logic

**Phase 2: Vision-Aware Retriever (10-14 hours)**

- [ ] **Session 2.1** (6-8h): VisionRetriever Class
  - Implement RetrievalResult, RetrievalResponse dataclasses
  - Implement VisionRetriever class
  - Text/image/hybrid query execution
  - Visual boosting logic
  - Result formatting
  - Test all query modes

- [ ] **Session 2.2** (4-6h): Reranking
  - Implement RerankerConfig dataclass
  - Implement VisualReranker class
  - Visual content boosting reranking
  - Layout complexity filtering
  - Test reranking logic

**Phase 3: Integration & Configuration (6-8 hours)**

- [ ] **Session 3.1** (3-4h): CLI Integration
  - Implement `query` command group
  - Implement `query text` command
  - Implement `query image` command
  - Implement `query hybrid` command
  - Result formatting and display

- [ ] **Session 3.2** (3-4h): Configuration Updates
  - Add RetrievalConfig to settings
  - Define configuration parameters
  - Environment variable mapping
  - Integrate config with components
  - Test configuration loading

**Phase 4: Testing (4-6 hours)**

- [ ] **Session 4.1** (2-3h): Unit Tests
  - Query processor tests
  - Vision retriever tests
  - Reranker tests
  - Achieve 85%+ coverage

- [ ] **Session 4.2** (2-3h): Integration Tests
  - End-to-end pipeline tests
  - CLI command tests
  - Performance benchmarks
  - Cross-platform validation

---

## Success Criteria

### Functional Requirements

- [ ] Text-only queries return results ranked by text similarity
- [ ] Image-only queries return results ranked by visual similarity
- [ ] Hybrid queries merge text + vision scores using RRF
- [ ] `boost_diagrams` flag increases scores for diagram-containing results
- [ ] `boost_tables` flag increases scores for table-containing results
- [ ] Visual reranker adjusts scores based on metadata
- [ ] CLI commands functional: `query text`, `query image`, `query hybrid`
- [ ] Query expansion generates visual-relevant keyword variants
- [ ] Configuration loaded from environment variables

### Performance Targets

- [ ] Text query latency: <200ms
- [ ] Image query latency: <500ms (including embedding generation)
- [ ] Hybrid query latency: <600ms
- [ ] Reranking overhead: <50ms for 100 results
- [ ] No performance regression from v0.5.0 baseline

### Quality Standards

- [ ] Type hints: 100% on all public methods
- [ ] Docstrings: British English, comprehensive
- [ ] Test coverage: 85%+ for retrieval module
- [ ] All tests pass on CPU (GPU tests skip gracefully)
- [ ] Linting: No errors from ruff or mypy

---

## Files Created/Modified

### New Files (Estimated ~1,450 lines)

**Source Code:**
- `src/retrieval/query_processor.py` (~200 lines) - Query type detection and processing
- `src/retrieval/query_expansion.py` (~150 lines) - Visual query expansion
- `src/retrieval/vision_retriever.py` (~400 lines) - Multi-modal retrieval engine
- `src/retrieval/reranker.py` (~150 lines) - Visual reranking logic
- `src/retrieval/__init__.py` (~20 lines) - Module exports

**Tests:**
- `tests/retrieval/test_query_processor.py` (~150 lines)
- `tests/retrieval/test_vision_retriever.py` (~200 lines)
- `tests/retrieval/test_reranker.py` (~100 lines)
- `tests/integration/test_vision_retrieval.py` (~150 lines)

### Modified Files

- `src/cli.py` (+100 lines) - Query commands
- `src/config/settings.py` (+30 lines) - RetrievalConfig
- `docs/guides/vision-queries.md` (new, ~100 lines) - Vision query guide

---

## Dependencies

### Python Packages

**Required (Already in v0.5.0):**
- torch >= 2.0.0 (ColPali embedder)
- transformers >= 4.35.0 (HuggingFace models)
- Pillow >= 10.0.0 (image processing)
- chromadb >= 0.4.0 (vector storage)
- click >= 8.0 (CLI framework)

**No New Dependencies** - Uses existing packages from v0.5.0.

### From Previous Versions

- v0.5.0: ColPaliEmbedder (for image query embedding)
- v0.5.0: DualEmbeddingStore (for multi-modal storage/retrieval)
- v0.4.x: TextEmbedder (for text query embedding)

---

## Known Limitations

**Image Query Latency:**
- Vision embedding generation adds 200-400ms per query image
- GPU required for reasonable performance (CPU 10x slower)
- Real-time applications may find image queries too slow

**Cross-Modal Alignment:**
- No learned alignment between text and vision embedding spaces
- Cannot directly compare text and vision similarity scores
- RRF fusion is heuristic-based, not optimised

**Query Expansion:**
- Text expansion uses heuristic keyword detection
- Not context-aware or learned from user behavior
- May generate irrelevant expansions for some queries

**Image Augmentation:**
- Disabled by default due to computational cost
- Each augmented variant requires re-embedding (expensive)
- Future enhancement: Pre-compute augmentations during ingestion

---

## Use Cases

### Example 1: Find Architecture Diagram

**User:** Upload sketch of desired architecture diagram

```bash
ragged query image architecture_sketch.png --num-results 5
```

**Result:** Returns 5 PDF pages with visually similar diagrams

---

### Example 2: Text Query with Diagram Preference

**User:** Search for "database schema" preferring results with diagrams

```bash
ragged query text "database schema" --boost-diagrams --num-results 10
```

**Result:** Returns 10 results, with diagram-containing pages ranked higher

---

### Example 3: Hybrid Query

**User:** Find pages about "authentication flow" similar to example diagram

```bash
ragged query hybrid "authentication flow" auth_example.png \
  --text-weight 0.6 --vision-weight 0.4 --num-results 10
```

**Result:** Returns 10 results combining text and visual similarity (60% text, 40% vision)

---

## Related Documentation

**Feature Implementation Details:**
- [VISION-003: Vision Retrieval](./features/VISION-003-vision-retrieval.md) - Complete implementation (~850 lines with code)

**Dependencies:**
- [v0.5.0: ColPali + Dual Storage](./v0.5.0.md) - Foundation for vision queries

**Context:**
- [v0.5.x Overview](./README.md) - Complete v0.5 series roadmap
- [Testing Specification](./testing.md) - Comprehensive test coverage

**Next Steps:**
- [v0.5.3: CLI Integration](./v0.5.3.md) - Enhanced CLI for all vision features
- [v0.5.4: Gradio Web UI](./v0.5.4.md) - Web interface for vision queries

---

**Status:** Planned - Ready for implementation
