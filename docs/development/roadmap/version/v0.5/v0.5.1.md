# v0.5.1: Infrastructure - GPU Resource Management

**Focus:** Robust GPU/CPU handling and intelligent resource management

**Hours:** 18-24 hours

**Dependencies:** v0.5.0 (ColPali + Dual Storage)

**Status:** Planned

---

## Overview

Version 0.5.1 implements comprehensive GPU resource management to ensure ragged handles diverse hardware environments gracefully. This version enhances the ColPali embedder from v0.5.0 with intelligent device detection, memory monitoring, OOM recovery, and adaptive batch sizing.

**What This Version Delivers:**
- Automatic device detection with fallback (CUDA > MPS > CPU)
- GPU memory monitoring with threshold-based alerts
- Out-of-memory (OOM) error recovery with multiple strategies
- Adaptive batch sizing based on available GPU memory
- Model lifecycle management for memory efficiency

**Why This Version is Critical:**
- Prevents crashes from GPU memory overflow
- Enables vision embeddings on systems with limited VRAM
- Provides graceful CPU fallback when GPU unavailable
- Optimises throughput by maximising batch sizes safely

**Builds On:** v0.5.0's ColPaliEmbedder foundation, adding production-ready resource management.

---

## Feature: VISION-004 GPU Resource Management (18-24 hours)

### Architecture Overview

**Device Priority:**
```
User Specified Device?
├─ YES → Use specified device
│  ├─ Device available? → Use it
│  └─ Device unavailable → Error (explicit choice failed)
│
└─ NO → Auto-detect
   ├─ CUDA available? → Use CUDA
   ├─ MPS available? → Use MPS
   └─ Default → CPU
```

**Memory Management Flow:**
```
Model Loading Request
    ↓
[Memory Check]
    ├─ Available memory >= required? → Load model
    └─ Insufficient memory
        ├─ Unload idle models → Retry
        └─ Still insufficient → Fallback to CPU
    ↓
[Batch Processing]
    ├─ Monitor memory during processing
    ├─ Detect OOM conditions
    └─ Reduce batch size if needed
    ↓
[Cleanup]
    └─ Unload model after idle timeout
```

---

## Implementation Plan

### Phase 1: Device Detection and Selection (4-6 hours)

#### Session 1.1: Device Manager (4-6h)

**Purpose:** Implement device detection, selection, and capability querying.

**Key Components:**
- `DeviceType` enum (CUDA, MPS, CPU)
- `DeviceInfo` dataclass (device metadata)
- `DeviceManager` class (detection and selection logic)

**Implementation Tasks:**
1. Detect all available compute devices (CUDA, MPS, CPU)
2. Query device properties (name, memory, compute capability)
3. Implement optimal device selection with user hints
4. Provide memory information queries
5. Add cache management (CUDA cache clearing)

**Deliverables:**
- `src/gpu/device_manager.py` (~300 lines)
- Multi-GPU support (CUDA device selection)
- Device information API
- Memory tracking for CUDA devices

**Success Criteria:**
- [ ] CUDA devices detected with full properties
- [ ] MPS (Apple Silicon) detected on compatible systems
- [ ] CPU fallback always available
- [ ] User-specified device hints respected
- [ ] Memory info accurate for CUDA devices

**Time:** 4-6 hours

---

### Phase 2: Memory Monitoring and OOM Handling (6-8 hours)

#### Session 2.1: Memory Monitor (3-4h)

**Purpose:** Track GPU memory usage and detect approaching OOM conditions.

**Key Components:**
- `MemorySnapshot` dataclass (point-in-time memory state)
- `MemoryMonitor` class (periodic snapshots and threshold alerts)

**Implementation Tasks:**
1. Implement memory snapshot capture (total, allocated, reserved, free)
2. Track memory history (last 100 snapshots)
3. Threshold-based callbacks (warning at 85%, critical at 95%)
4. Recommend batch size adjustments based on observed utilisation
5. Memory utilisation percentage calculation

**Deliverables:**
- `src/gpu/memory_monitor.py` (~200 lines)
- Memory snapshot tracking
- Configurable threshold alerts
- Batch size recommendation system

**Success Criteria:**
- [ ] Memory snapshots accurate (<10ms overhead)
- [ ] Warning callback triggered at 85% utilisation
- [ ] Critical callback triggered at 95% utilisation
- [ ] Batch size recommendations improve memory utilisation
- [ ] Snapshot history limited to prevent memory leak

**Time:** 3-4 hours

---

#### Session 2.2: OOM Recovery (3-4h)

**Purpose:** Handle out-of-memory errors with automatic recovery strategies.

**Key Components:**
- `OOMHandler` class (multi-strategy OOM recovery)
- `@with_oom_handling` decorator (automatic OOM handling)

**Recovery Strategies (Applied Sequentially):**
1. **Strategy 1:** Clear GPU cache and retry
2. **Strategy 2:** Reduce batch size by 50% and retry
3. **Strategy 3:** Fallback to CPU and retry

**Implementation Tasks:**
1. Detect OOM errors from RuntimeError messages
2. Implement cache clearing strategy
3. Implement batch size reduction strategy
4. Implement CPU fallback strategy
5. Create decorator for easy integration

**Deliverables:**
- `src/gpu/oom_handler.py` (~200 lines)
- Multi-strategy OOM recovery
- Configurable strategy enable/disable
- Decorator for function-level OOM handling

**Success Criteria:**
- [ ] OOM errors detected reliably
- [ ] Cache clearing recovers ~30% of cases
- [ ] Batch reduction recovers ~50% of remaining cases
- [ ] CPU fallback always succeeds (if enabled)
- [ ] Maximum 3 retry attempts (prevents infinite loops)

**Time:** 3-4 hours

---

### Phase 3: Adaptive Batch Sizing (4-6 hours)

#### Session 3.1: Batch Size Calculator (2-3h)

**Purpose:** Calculate optimal batch sizes based on GPU memory constraints.

**Key Components:**
- `BatchSizeConfig` dataclass (batch sizing parameters)
- `AdaptiveBatchSizer` class (memory-aware batch size calculation)

**Implementation Tasks:**
1. Estimate memory per batch item (embeddings, activations, overhead)
2. Calculate batch size from available GPU memory
3. Apply safety margin (10%) to prevent borderline OOM
4. Clamp to min/max bounds (1-32 default)
5. Adaptive adjustment based on MemoryMonitor feedback

**Calculation Formula:**
```
bytes_per_item = embedding_dim × sequence_length × bytes_per_element × overhead_factor
available_memory = total_memory × (target_utilisation - safety_margin)
batch_size = floor(available_memory / bytes_per_item)
batch_size = clamp(batch_size, min_batch_size, max_batch_size)
```

**Deliverables:**
- `src/gpu/batch_sizer.py` (~150 lines)
- Memory-based batch size calculation
- Adaptive adjustment from memory feedback
- Configurable parameters (min/max, target utilisation, safety margin)

**Success Criteria:**
- [ ] Batch sizes fit in available memory (no OOM)
- [ ] Target 75% memory utilisation (balance safety vs throughput)
- [ ] Adaptive adjustment improves utilisation over time
- [ ] CPU uses maximum batch size (no memory constraint)

**Time:** 2-3 hours

---

#### Session 3.2: Integration with ColPali Embedder (2-3h)

**Purpose:** Integrate GPU management into existing ColPaliEmbedder from v0.5.0.

**Updates to ColPaliEmbedder:**
1. Add `DeviceManager` for device selection
2. Add `MemoryMonitor` for GPU devices
3. Add `AdaptiveBatchSizer` for intelligent batch sizing
4. Add `OOMHandler` for error recovery
5. Update `embed_batch` with memory monitoring and OOM handling

**New Parameters:**
- `enable_adaptive_batching` (default: True)
- Batch size auto-calculation if not specified
- Memory snapshots before/after batch processing

**Deliverables:**
- Updated `src/embeddings/colpali_embedder.py` (+100 lines)
- GPU management fully integrated
- Backward compatibility maintained (optional features)

**Success Criteria:**
- [ ] ColPaliEmbedder works with adaptive batching
- [ ] Memory monitoring integrated seamlessly
- [ ] OOM recovery prevents crashes
- [ ] Performance impact <5% overhead

**Time:** 2-3 hours

---

### Phase 4: Testing and Documentation (4-6 hours)

#### Session 4.1: Unit Tests (2-3h)

**Test Coverage:**

**Device Manager Tests:**
- Device detection (CUDA, MPS, CPU)
- Optimal device selection (auto and manual)
- Memory information queries (CUDA)
- Cache clearing functionality
- Multi-GPU device selection

**Memory Monitor Tests:**
- Memory snapshot capture
- Threshold callback triggering
- Batch size recommendations
- Snapshot history management

**OOM Handler Tests:**
- OOM error detection
- Cache clearing strategy
- Batch reduction strategy
- CPU fallback strategy
- Multiple retry attempts

**Batch Sizer Tests:**
- Batch size calculation from memory
- Adaptive adjustment
- Min/max clamping
- CPU batch sizing

**Example Test:**
```python
# tests/gpu/test_device_manager.py

import pytest
from ragged.gpu.device_manager import DeviceManager, DeviceType

class TestDeviceManager:
    """Test device detection and management."""

    def test_device_detection(self):
        """Test automatic device detection."""
        manager = DeviceManager()
        assert len(manager._available_devices) > 0

        # Should have at least CPU
        device_types = [d.device_type for d in manager._available_devices]
        assert DeviceType.CPU in device_types

    def test_optimal_device_selection(self):
        """Test optimal device selection logic."""
        manager = DeviceManager()

        # Auto-select (should prefer GPU)
        device = manager.get_optimal_device()
        assert device is not None

        # Explicit CPU selection
        cpu_device = manager.get_optimal_device(device_hint="cpu")
        assert cpu_device.device_type == DeviceType.CPU

    @pytest.mark.skipif(not torch.cuda.is_available(), reason="Requires CUDA")
    def test_cuda_memory_query(self):
        """Test CUDA memory information retrieval."""
        manager = DeviceManager()
        cuda_device = manager.get_optimal_device(device_hint="cuda")

        memory_info = manager.get_device_memory_info(cuda_device)
        assert memory_info["total"] > 0
        assert memory_info["free"] <= memory_info["total"]
```

**Deliverables:**
- `tests/gpu/test_device_manager.py` (~150 lines)
- `tests/gpu/test_memory_monitor.py` (~100 lines)
- `tests/gpu/test_oom_handler.py` (~100 lines)
- `tests/gpu/test_batch_sizer.py` (~100 lines)

**Time:** 2-3 hours

---

#### Session 4.2: Integration Tests and Documentation (2-3h)

**Integration Tests:**
- End-to-end GPU resource management workflow
- OOM recovery in real ColPali embedding generation
- Adaptive batch sizing with actual vision models
- Multi-GPU device selection (if hardware available)
- Cross-platform compatibility (CUDA, MPS, CPU)

**Documentation:**
- GPU setup guide (CUDA installation, driver requirements)
- Memory management best practices
- Troubleshooting OOM errors
- Performance tuning (batch size configuration)

**Deliverables:**
- `tests/integration/test_gpu_management.py` (~150 lines)
- `docs/guides/gpu-configuration.md` (~100 lines)
- Updated `docs/installation.md` with GPU requirements

**Time:** 2-3 hours

---

## Implementation Checklist

### Session-by-Session Breakdown

**Phase 1: Device Detection (4-6 hours)**

- [ ] **Session 1.1** (4-6h): Device Manager implementation
  - Implement DeviceType enum and DeviceInfo dataclass
  - Implement DeviceManager._detect_available_devices
  - Implement DeviceManager.get_optimal_device
  - Implement DeviceManager.get_device_memory_info
  - Implement DeviceManager.clear_cache
  - Test CUDA, MPS, CPU detection

**Phase 2: Memory Monitoring and OOM (6-8 hours)**

- [ ] **Session 2.1** (3-4h): Memory Monitor implementation
  - Implement MemorySnapshot dataclass
  - Implement MemoryMonitor.take_snapshot
  - Implement threshold checking and callbacks
  - Implement batch size recommendation
  - Test snapshot accuracy and callbacks

- [ ] **Session 2.2** (3-4h): OOM Handler implementation
  - Implement OOMHandler class
  - Implement handle_oom with 3-strategy recovery
  - Implement @with_oom_handling decorator
  - Test each recovery strategy
  - Test fallback chain (cache → batch → CPU)

**Phase 3: Adaptive Batch Sizing (4-6 hours)**

- [ ] **Session 3.1** (2-3h): Batch Size Calculator
  - Implement BatchSizeConfig dataclass
  - Implement AdaptiveBatchSizer.calculate_batch_size
  - Implement AdaptiveBatchSizer.adjust_batch_size
  - Test batch size calculations
  - Test adaptive adjustment

- [ ] **Session 3.2** (2-3h): ColPali Integration
  - Add GPU management to ColPaliEmbedder.__init__
  - Update ColPaliEmbedder.embed_batch with monitoring
  - Integrate OOMHandler
  - Test with real vision embeddings
  - Verify backward compatibility

**Phase 4: Testing and Documentation (4-6 hours)**

- [ ] **Session 4.1** (2-3h): Unit Tests
  - Device Manager test suite
  - Memory Monitor test suite
  - OOM Handler test suite
  - Batch Sizer test suite
  - Achieve 85%+ coverage

- [ ] **Session 4.2** (2-3h): Integration Tests and Docs
  - End-to-end GPU management test
  - OOM recovery integration test
  - Write GPU configuration guide
  - Write troubleshooting guide
  - Update installation docs

---

## Success Criteria

### Functional Requirements

- [ ] Device detection: CUDA > MPS > CPU priority order
- [ ] Automatic fallback to CPU if GPU unavailable
- [ ] Memory monitoring: Warning at 85%, critical at 95% utilisation
- [ ] OOM recovery: 3-strategy fallback (cache → batch → CPU)
- [ ] Adaptive batch sizing: Calculated from available memory
- [ ] Model lifecycle: Efficient loading/unloading
- [ ] ColPali embedder enhanced without breaking changes

### Performance Targets

- [ ] Device detection: <100ms
- [ ] Memory snapshot: <10ms per snapshot
- [ ] OOM recovery: <2s total retry time (all strategies)
- [ ] Adaptive batch sizing: <50ms calculation time
- [ ] Embedding throughput: No more than 5% overhead from monitoring

### Quality Standards

- [ ] Type hints: 100% on all public methods
- [ ] Docstrings: British English, comprehensive
- [ ] Test coverage: 85%+ for GPU module
- [ ] Cross-platform: Works on CUDA, MPS, and CPU-only systems
- [ ] No crashes on OOM: Graceful degradation always

### Compatibility

- [ ] Works with v0.5.0 ColPaliEmbedder
- [ ] Backward compatible: Can disable GPU management features
- [ ] Multi-GPU: Supports CUDA device selection (cuda:0, cuda:1, etc.)
- [ ] CPU-only: Full functionality without GPU

---

## Files Created/Modified

### New Files (Estimated ~1,200 lines)

**Source Code:**
- `src/gpu/device_manager.py` (~300 lines) - Device detection and selection
- `src/gpu/memory_monitor.py` (~200 lines) - Memory monitoring and snapshots
- `src/gpu/oom_handler.py` (~200 lines) - OOM recovery strategies
- `src/gpu/batch_sizer.py` (~150 lines) - Adaptive batch sizing
- `src/gpu/__init__.py` (~20 lines) - Module exports

**Tests:**
- `tests/gpu/test_device_manager.py` (~150 lines)
- `tests/gpu/test_memory_monitor.py` (~100 lines)
- `tests/gpu/test_oom_handler.py` (~100 lines)
- `tests/gpu/test_batch_sizer.py` (~100 lines)
- `tests/integration/test_gpu_management.py` (~150 lines)

**Documentation:**
- `docs/guides/gpu-configuration.md` (~100 lines) - GPU setup guide
- `docs/guides/troubleshooting-oom.md` (~80 lines) - OOM troubleshooting

### Modified Files

- `src/embeddings/colpali_embedder.py` (+100 lines) - GPU management integration
- `docs/installation.md` (+50 lines) - GPU requirements and setup
- `pyproject.toml` (+5 lines) - Optional GPU monitoring dependencies

---

## Dependencies

### Python Packages

**Required (Already in v0.5.0):**
- torch >= 2.0.0 (GPU operations)
- loguru >= 0.7.0 (logging)

**No New Dependencies** - Uses existing torch and loguru packages.

### System Dependencies

**For CUDA (NVIDIA GPUs):**
- CUDA Toolkit 11.8+ (for CUDA devices)
- Compatible NVIDIA drivers

**For MPS (Apple Silicon):**
- macOS 12.3+ (for MPS support)
- Apple Silicon M1/M2/M3/M4

**For CPU:**
- No additional dependencies

### From Previous Versions

- v0.5.0: ColPaliEmbedder class (to be enhanced)

---

## Hardware Requirements

### Recommended Configuration

**NVIDIA GPU:**
- 8GB+ VRAM (allows batch size ~12 pages)
- CUDA Compute Capability 7.0+ (RTX 20-series or newer)

**Apple Silicon:**
- M1/M2/M3 with 16GB+ unified memory
- macOS 12.3 or later

**CPU-Only:**
- 16GB+ system RAM
- Modern multi-core CPU (8+ cores recommended)

### Minimum Configuration

**NVIDIA GPU:**
- 4GB VRAM (batch size ~4 pages)
- CUDA Compute Capability 6.0+

**Apple Silicon:**
- M1 with 8GB unified memory (limited batch sizes)

**CPU-Only:**
- 8GB system RAM (slow, 10x+ slower than GPU)

---

## Known Limitations

**MPS Memory Tracking:**
- Apple Silicon MPS doesn't expose detailed memory statistics via PyTorch
- Memory monitor returns placeholder values for MPS devices
- Future enhancement: Metal API integration for accurate MPS memory tracking

**Multi-GPU Load Balancing:**
- Not implemented in this version
- Single device per ColPaliEmbedder instance
- Manual device selection for multi-GPU systems (e.g., "cuda:1")

**Memory Estimation Accuracy:**
- Overhead factor (3.0x) is heuristic-based, not model-specific
- May be conservative or aggressive depending on actual model
- Adaptive adjustment compensates over time

**OOM Recovery Guarantee:**
- Cannot guarantee recovery for all OOM scenarios
- Persistent OOM after all strategies indicates insufficient hardware
- CPU fallback is last resort (significantly slower)

---

## Performance Impact

### GPU Management Overhead

**Device Detection:** ~100ms (one-time, during initialisation)
**Memory Snapshot:** ~10ms per snapshot
**Batch Size Calculation:** ~50ms (one-time or periodic)
**OOM Handling:** ~2s total (only on OOM error)

**Expected Throughput Impact:** <5% under normal operation (no OOM)

### Memory Efficiency Gains

**Adaptive Batch Sizing:**
- Baseline: Fixed batch size 4 (conservative, ~40% VRAM utilised)
- Adaptive: Dynamic batch size 8-12 (target 75% VRAM utilised)
- **Throughput Increase:** ~2-3x from larger batches

**OOM Prevention:**
- Baseline: Crash on OOM, manual batch size tuning required
- With OOM Handler: Automatic recovery, no crashes
- **Reliability:** Near-100% uptime (CPU fallback always available)

---

## Testing Strategy

### Unit Tests (450 lines, 2-3 hours)

**Coverage by Component:**
- Device Manager: Device detection, selection, memory queries
- Memory Monitor: Snapshots, threshold callbacks, recommendations
- OOM Handler: Error detection, recovery strategies, fallback chain
- Batch Sizer: Calculation, adaptive adjustment, constraints

**Test Markers:**
- `@pytest.mark.skipif(not torch.cuda.is_available())` - CUDA-only tests
- `@pytest.mark.skipif(not torch.backends.mps.is_available())` - MPS-only tests
- All tests run on CPU-only systems (GPU tests skip gracefully)

### Integration Tests (150 lines, 2-3 hours)

**Scenarios:**
1. Full vision embedding pipeline with GPU management
2. OOM recovery during large batch processing
3. Adaptive batch sizing improves throughput over time
4. Multi-GPU device selection (if hardware available)
5. Cross-platform compatibility (CUDA, MPS, CPU)

### Manual Testing (v0.5.6)

**Deferred to Documentation version:**
- Test on diverse GPU hardware (low-end, mid-range, high-end)
- Test on CPU-only systems (various core counts)
- Test OOM scenarios with intentionally small VRAM
- Benchmark throughput improvements from adaptive batching

---

## Related Documentation

**Feature Implementation Details:**
- [VISION-004: GPU Resource Management](./features/VISION-004-gpu-management.md) - Complete implementation (~900 lines with code)

**Dependencies:**
- [v0.5.0: ColPali + Dual Storage](./v0.5.0.md) - Foundation for GPU management

**Context:**
- [v0.5.x Overview](./README.md) - Complete v0.5 series roadmap
- [Testing Specification](./testing.md) - Comprehensive test coverage

**Next Steps:**
- [v0.5.2: Vision Retrieval](./v0.5.2.md) - Multi-modal queries (benefits from v0.5.1 GPU management)
- [v0.5.3: CLI Integration](./v0.5.3.md) - Command-line interface (exposes GPU management features)

---

**Status:** Planned - Ready for implementation
