# v0.3.2 - Advanced Query Processing

**Category:** Core Retrieval Functionality

**Estimated Time:** 53-55 hours

**Status:** Planned

---

## Overview

Implement state-of-the-art retrieval techniques for 10-20% quality improvement per technique.

**Target:** MRR@10 > 0.75 (from ~0.60)

**Strategy:** Incremental improvements with each technique measured against RAGAS baseline from v0.3.0.

---

## Prerequisites

**Security & Privacy Foundation (REQUIRED):**

All v0.3.x versions depend on the security and privacy infrastructure established in v0.2.10 and v0.2.11. These versions MUST be completed before implementing any v0.3.x features.

- ✅ **v0.2.10 (Security Hardening)** - Eliminates Pickle vulnerabilities, implements session isolation, establishes security testing framework
- ✅ **v0.2.11 (Privacy Infrastructure)** - Provides encryption at rest, PII detection/redaction, data lifecycle management, GDPR compliance

**Why Required:** v0.3.x features will store and process user data (metrics, REPL history, API requests). The security and privacy foundations ensure this data is protected from the start.

---

## Features

### FEAT-001: Query Decomposition (15h)

**Priority:** High
**Dependencies:** v0.3.0 (RAGAS metrics)

#### Scope

Break complex multi-part queries into sub-queries, retrieve for each, then merge results.

**Example:**
- Query: "What methods did the authors use and how do they compare to prior work?"
- Decomposed:
  1. "What methods did the authors use?"
  2. "What was the prior work?"
  3. "How do the methods compare?"

#### Implementation Overview

- **Module:** `src/retrieval/query_decomposition.py`
- **Approach:** LLM-based decomposition with prompt engineering
- **Integration:** Optional pipeline stage (controlled by config personas)
- **Merging Strategy:** Weighted combination with deduplication

#### Key Components

- `QueryDecomposer` class with LLM integration
- Sub-query generation with validation
- Result merging with relevance scoring
- Caching to avoid repeated decomposition

---

### FEAT-002: HyDE - Hypothetical Document Embeddings (8h)

**Priority:** Medium
**Dependencies:** None

#### Scope

Generate hypothetical answer to query, embed it, use for retrieval (better semantic match than query embedding).

**Rationale:** Answers are semantically closer to document chunks than questions.

#### Implementation Overview

- **Module:** `src/retrieval/hyde.py`
- **Approach:** LLM generates hypothetical answer → embed → retrieve
- **Integration:** Optional pre-retrieval stage
- **Fallback:** Use original query if HyDE generation fails

#### Key Components

- `HyDEGenerator` class
- Hypothetical document generation prompts
- Embedding and retrieval integration
- Quality validation (detect hallucinations)

---

### FEAT-003: Reranking with Cross-Encoders (6h)

**Priority:** Critical
**Dependencies:** None (already partially implemented)

#### Scope

Enhance existing reranking with state-of-the-art cross-encoder models for improved top-k precision.

**Current:** Basic reranking exists
**Enhancement:** Better models, configurable rerank-to ratio

#### Implementation Overview

- **Module:** `src/retrieval/reranker.py` (enhance existing)
- **Models:** `cross-encoder/ms-marco-MiniLM-L-6-v2` (default), configurable
- **Integration:** Post-retrieval stage
- **Performance:** ~2s for top-50 → top-5

#### Key Components

- Enhanced `Reranker` class
- Model loading and caching
- Batch processing for efficiency
- Configurable rerank ratio (top_k → rerank_to)

---

### FEAT-004: Contextual Compression (12h)

**Priority:** High
**Dependencies:** None

#### Scope

Extract only relevant sentences/paragraphs from retrieved chunks, reducing noise and improving context quality.

**Benefit:** Feed LLM only the relevant portions, improving answer quality and reducing token usage.

#### Implementation Overview

- **Module:** `src/retrieval/compression.py`
- **Approach:** Sentence-level relevance scoring
- **Integration:** Post-reranking stage
- **Compression Ratio:** Target 30-50% reduction

#### Key Components

- `ContextualCompressor` class
- Sentence extraction with relevance scoring
- Coherence preservation (keep surrounding context)
- Compression ratio validation

---

## Implementation Phases

### Phase 1: Design & Architecture (8-10h)

**Sessions 1-2:**
- Use architecture-advisor agent for retrieval pipeline design
- Design query decomposition strategy
- Design HyDE prompts and validation
- Design compression algorithms
- Document architecture decisions

**Deliverables:**
- Architecture decision record
- Pipeline design documented
- Prompt templates prepared

### Phase 2: Core Implementation (35-40h)

**Sessions 3-4: Query Decomposition (15h)**
- Implement `QueryDecomposer` class
- LLM integration for sub-query generation
- Result merging algorithm
- Deduplication logic
- Caching mechanism

**Sessions 5-6: HyDE (8h)**
- Implement `HyDEGenerator` class
- Hypothetical document generation
- Embedding integration
- Quality validation
- Fallback handling

**Sessions 7: Reranking Enhancement (6h)**
- Enhance existing `Reranker` class
- Add cross-encoder models
- Implement batch processing
- Add configurable ratios
- Performance optimisation

**Sessions 8-10: Contextual Compression (12h)**
- Implement `ContextualCompressor` class
- Sentence-level relevance scoring
- Coherence preservation logic
- Compression ratio tuning
- Quality validation

**Deliverables:**
- All 4 features implemented
- Integration with existing pipeline
- Configuration options added

### Phase 3: Testing & Validation (8-10h)

**Sessions 11-12:**
- Unit tests for all 4 features
- Integration testing
- RAGAS evaluation with each technique
- Measure improvement per feature
- Performance profiling

**Deliverables:**
- 100% test coverage
- RAGAS scores documented
- Performance benchmarks

### Phase 4: Documentation & Release (2-4h)

**Session 13:**
- Use documentation-architect agent
- Document all features in user guides
- Use documentation-auditor agent
- Use git-documentation-committer agent
- Tag v0.3.2 release

**Deliverables:**
- Complete documentation
- Release tagged

---

## Technical Architecture

### Module Structure

```
src/retrieval/
├── query_decomposition.py      # Query splitting (200 lines)
│   └── class QueryDecomposer
├── hyde.py                      # Hypothetical docs (150 lines)
│   └── class HyDEGenerator
├── reranker.py                  # Enhanced reranking (200 lines)
│   └── class Reranker
├── compression.py               # Context compression (250 lines)
│   └── class ContextualCompressor
└── retriever.py                 # Pipeline orchestration (modified)

tests/retrieval/
├── test_query_decomposition.py  # Decomposition tests
├── test_hyde.py                 # HyDE tests
├── test_reranker.py             # Reranking tests
└── test_compression.py          # Compression tests
```

### Retrieval Pipeline Flow

```
User Query
    ↓
[Optional] Query Decomposition
    ├─ Sub-query 1 ─┐
    ├─ Sub-query 2 ─┼─ Individual retrievals
    └─ Sub-query 3 ─┘
    ↓ Merge & deduplicate
[Optional] HyDE Enhancement
    ↓ Generate hypothetical answer → embed
Initial Retrieval (hybrid)
    ↓ Top-K chunks (e.g., 50)
Reranking (cross-encoder)
    ↓ Top-rerank_to chunks (e.g., 10)
[Optional] Contextual Compression
    ↓ Extract relevant sentences
Final Chunks → LLM Generation
```

### Configuration Integration

```python
# Config personas control which stages are enabled
# "accuracy" persona:
config.enable_query_decomposition = True
config.enable_hyde = False
config.enable_reranking = True
config.enable_compression = True

# "speed" persona:
config.enable_query_decomposition = False
config.enable_hyde = False
config.enable_reranking = False
config.enable_compression = False
```

---

## Risk Analysis & Mitigation

**Risk 1: Query Decomposition Quality**
- **Impact:** High - Poor decomposition reduces quality
- **Probability:** Medium - LLM outputs vary
- **Mitigation:** Validate sub-queries, fallback to original query, test with diverse queries
- **Detection:** RAGAS scores, manual review

**Risk 2: HyDE Hallucinations**
- **Impact:** Medium - Hypothetical docs may be inaccurate
- **Probability:** Medium - LLMs hallucinate
- **Mitigation:** Detect low-confidence generations, fallback to original query, validate with retrieval scores
- **Detection:** Confidence thresholds, quality checks

**Risk 3: Performance Overhead**
- **Impact:** Medium - Each technique adds latency
- **Probability:** High - Multiple LLM calls
- **Mitigation:** Make techniques optional (personas), cache results, batch processing, async where possible
- **Detection:** Performance profiling

**Risk 4: Compression Loses Context**
- **Impact:** Medium - Over-compression reduces answer quality
- **Probability:** Medium - Balance precision/recall
- **Mitigation:** Tune compression ratios, preserve surrounding sentences, validate with RAGAS
- **Detection:** RAGAS faithfulness scores

---

## Quality Gates

### Functional Requirements
- [ ] Query decomposition splits complex queries correctly
- [ ] Sub-query merging deduplicates and ranks properly
- [ ] HyDE generates relevant hypothetical documents
- [ ] HyDE fallback works when generation fails
- [ ] Reranking improves top-k precision
- [ ] Cross-encoder models load and cache correctly
- [ ] Contextual compression extracts relevant sentences
- [ ] Compression preserves coherence
- [ ] All features integrate with config personas
- [ ] Pipeline stages can be enabled/disabled

### Performance Requirements
- [ ] Query decomposition adds <2s latency
- [ ] HyDE adds <1.5s latency
- [ ] Reranking (top-50 → top-5) completes in <2s
- [ ] Contextual compression adds <1s latency
- [ ] Full "accuracy" persona pipeline <15s total

### Quality Requirements
- [ ] MRR@10 > 0.75 (target achieved)
- [ ] Each technique shows 5-15% improvement
- [ ] RAGAS Context Precision > 0.75
- [ ] RAGAS Context Recall > 0.75
- [ ] No quality regression when techniques disabled

### Code Quality Requirements
- [ ] 100% test coverage for new code
- [ ] All unit tests passing
- [ ] Integration tests for each technique
- [ ] Type hints complete
- [ ] Docstrings complete (British English)

---

## Execution Checklist

### Pre-Implementation
- [ ] Create feature branch: `git checkout -b feature/v0.3.2-advanced-query-processing`
- [ ] Review RAGAS baseline from v0.3.1
- [ ] Research cross-encoder models
- [ ] Research compression techniques
- [ ] Use architecture-advisor agent for pipeline design

### Implementation (condensed)

**Query Decomposition:**
- [ ] Implement `QueryDecomposer` class
- [ ] Add LLM prompts for decomposition
- [ ] Implement sub-query merging
- [ ] Add caching
- [ ] Unit tests

**HyDE:**
- [ ] Implement `HyDEGenerator` class
- [ ] Add hypothetical document prompts
- [ ] Integrate with embedding
- [ ] Add quality validation
- [ ] Unit tests

**Reranking Enhancement:**
- [ ] Enhance `Reranker` class
- [ ] Add cross-encoder models
- [ ] Implement batch processing
- [ ] Add configuration options
- [ ] Unit tests

**Contextual Compression:**
- [ ] Implement `ContextualCompressor` class
- [ ] Add sentence extraction logic
- [ ] Implement coherence preservation
- [ ] Tune compression ratios
- [ ] Unit tests

### Testing & Validation
- [ ] Run RAGAS with each technique enabled
- [ ] Measure per-technique improvement
- [ ] Performance profiling
- [ ] Integration testing
- [ ] Validate MRR@10 > 0.75 target

### Documentation & Release
- [ ] Use documentation-architect agent
- [ ] Document all 4 techniques in user guides
- [ ] Add examples and use cases
- [ ] Run documentation-auditor agent
- [ ] Use git-documentation-committer agent
- [ ] Tag v0.3.2 release

### Post-Implementation
- [ ] Update implementation record
- [ ] Record actual hours vs estimate
- [ ] Document lessons learned
- [ ] Update v0.3/README.md progress

---

## Agent Workflow (10-12h)

1. **architecture-advisor (2h):** Retrieval pipeline design review
2. **documentation-architect (2h):** Pipeline documentation structure
3. **documentation-auditor (3-4h):** Comprehensive review
4. **git-documentation-committer (3-4h):** Commit

---

## Deliverables

1. **Query Decomposition** - Complex queries → sub-queries → merged results
2. **HyDE** - Hypothetical document embeddings for better retrieval
3. **Enhanced Reranking** - Cross-encoder models with configurable ratios
4. **Contextual Compression** - Extract relevant sentences, reduce noise
5. **Integrated Pipeline** - All techniques work together, controlled by personas
6. **Quality Improvement** - MRR@10 > 0.75 achieved

---

## Success Criteria

- ✅ MRR@10 > 0.75 (from ~0.60) - **25% improvement**
- ✅ Each technique shows measurable improvement (5-15%)
- ✅ RAGAS Context Precision > 0.75
- ✅ RAGAS Context Recall > 0.75
- ✅ Performance acceptable for all personas (<15s for "accuracy")
- ✅ Techniques can be enabled/disabled via configuration
- ✅ All tests passing, documentation complete

---

## Related Documentation

- [v0.3.0 Roadmap](./README.md) - Overview
- [v0.3.0 - Foundation & Metrics](./v0.3.0.md) - RAGAS baseline
- [v0.3.0 - Configuration Transparency](./v0.3.0.md) - Persona system
- [Query Processing Features](./features/query-processing.md) - Detailed specifications

---

**Status:** Planned
