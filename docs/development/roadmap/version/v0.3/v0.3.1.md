# v0.3.1 - Foundation & Metrics

**Category:** Quality & Evaluation Infrastructure

**Estimated Time:** 30 hours

**Status:** Planned

---

## Overview

Establish objective quality metrics BEFORE implementing any improvements. This enables data-driven development by providing baseline scores for comparison.

**Why First:** Measuring quality improvement requires baselines. Without metrics, we can't validate that v0.3.x improvements actually work.

---

## Prerequisites

**Security & Privacy Foundation (REQUIRED):**

All v0.3.x versions depend on the security and privacy infrastructure established in v0.2.10 and v0.2.11. These versions MUST be completed before implementing any v0.3.x features.

- ✅ **v0.2.10 (Security Hardening)** - Eliminates Pickle vulnerabilities, implements session isolation, establishes security testing framework
- ✅ **v0.2.11 (Privacy Infrastructure)** - Provides encryption at rest, PII detection/redaction, data lifecycle management, GDPR compliance

**Why Required:** v0.3.x features will store and process user data (metrics, REPL history, API requests). The security and privacy foundations ensure this data is protected from the start.

---

## Features

### FEAT-009: RAGAS Evaluation Framework (16h)

**Priority:** Critical
**Dependencies:** None

#### Scope

Integrate RAGAS (Retrieval-Augmented Generation Assessment) framework for automated quality evaluation.

**Metrics to Track:**
1. **Context Precision:** Are retrieved chunks relevant?
2. **Context Recall:** Did we retrieve all relevant chunks?
3. **Faithfulness:** Does answer match context?
4. **Answer Relevance:** Does answer address the question?

#### Implementation

```python
# src/evaluation/ragas_evaluator.py (NEW FILE)
"""RAGAS-based quality evaluation."""
from typing import List, Dict
from dataclasses import dataclass
from ragas import evaluate
from ragas.metrics import (
    context_precision,
    context_recall,
    faithfulness,
    answer_relevancy
)

@dataclass
class EvaluationResult:
    """Results of RAGAS evaluation."""
    context_precision: float
    context_recall: float
    faithfulness: float
    answer_relevancy: float
    overall_score: float

    def to_dict(self) -> Dict:
        """Convert to dictionary."""
        return {
            'context_precision': self.context_precision,
            'context_recall': self.context_recall,
            'faithfulness': self.faithfulness,
            'answer_relevancy': self.answer_relevancy,
            'overall_score': self.overall_score
        }

class RAGASEvaluator:
    """
    Evaluate RAG pipeline quality using RAGAS framework.

    Provides automated, objective quality metrics.
    """

    def __init__(self):
        """Initialise evaluator with default metrics."""
        self.metrics = [
            context_precision,
            context_recall,
            faithfulness,
            answer_relevancy
        ]

    def evaluate(
        self,
        questions: List[str],
        answers: List[str],
        contexts: List[List[str]],
        ground_truths: List[str] = None
    ) -> EvaluationResult:
        """
        Evaluate RAG pipeline performance.

        Args:
            questions: User queries
            answers: Generated answers
            contexts: Retrieved chunks for each query
            ground_truths: Optional reference answers

        Returns:
            EvaluationResult with metric scores
        """
        # Prepare dataset
        dataset = {
            'question': questions,
            'answer': answers,
            'contexts': contexts
        }

        if ground_truths:
            dataset['ground_truth'] = ground_truths

        # Run evaluation
        result = evaluate(
            dataset,
            metrics=self.metrics
        )

        # Calculate overall score
        scores = [
            result['context_precision'],
            result['context_recall'],
            result['faithfulness'],
            result['answer_relevancy']
        ]
        overall = sum(scores) / len(scores)

        return EvaluationResult(
            context_precision=result['context_precision'],
            context_recall=result['context_recall'],
            faithfulness=result['faithfulness'],
            answer_relevancy=result['answer_relevancy'],
            overall_score=overall
        )

    def evaluate_single(
        self,
        question: str,
        answer: str,
        contexts: List[str],
        ground_truth: str = None
    ) -> EvaluationResult:
        """Evaluate a single query-answer pair."""
        return self.evaluate(
            questions=[question],
            answers=[answer],
            contexts=[contexts],
            ground_truths=[ground_truth] if ground_truth else None
        )
```

**CLI Integration:**
```python
# src/cli/commands/evaluate.py (NEW FILE)
"""Quality evaluation commands."""
import click
from src.evaluation.ragas_evaluator import RAGASEvaluator

@click.group()
def evaluate():
    """Evaluate RAG pipeline quality."""
    pass

@evaluate.command()
@click.argument('test_set', type=click.Path(exists=True))
@click.option('--format', type=click.Choice(['table', 'json']), default='table')
def ragas(test_set, format):
    """
    Evaluate using RAGAS framework.

    TEST_SET: Path to JSON file with test questions/answers

    Expected format:
    [
        {
            "question": "What is RAG?",
            "ground_truth": "Retrieval-Augmented Generation..."
        }
    ]
    """
    import json
    from src.storage.vector_store import VectorStore
    from src.retrieval.retriever import Retriever
    from src.generation.ollama_client import OllamaClient

    # Load test set
    with open(test_set) as f:
        tests = json.load(f)

    # Initialize components
    store = VectorStore()
    retriever = Retriever(vector_store=store)
    llm = OllamaClient()
    evaluator = RAGASEvaluator()

    # Run queries
    questions = [t['question'] for t in tests]
    ground_truths = [t.get('ground_truth') for t in tests]

    answers = []
    contexts = []

    for question in questions:
        # Retrieve
        chunks = retriever.retrieve(question, top_k=5)
        context = [c.content for c in chunks]
        contexts.append(context)

        # Generate
        answer = llm.generate(
            query=question,
            context='\n\n'.join(context)
        )
        answers.append(answer)

    # Evaluate
    result = evaluator.evaluate(
        questions=questions,
        answers=answers,
        contexts=contexts,
        ground_truths=ground_truths if any(ground_truths) else None
    )

    # Display results
    if format == 'table':
        click.echo("\nRAGAS Evaluation Results:")
        click.echo(f"  Context Precision: {result.context_precision:.3f}")
        click.echo(f"  Context Recall:    {result.context_recall:.3f}")
        click.echo(f"  Faithfulness:      {result.faithfulness:.3f}")
        click.echo(f"  Answer Relevancy:  {result.answer_relevancy:.3f}")
        click.echo(f"  Overall Score:     {result.overall_score:.3f}")
    else:
        click.echo(json.dumps(result.to_dict(), indent=2))
```

#### Testing Requirements

- [ ] Test RAGAS integration with sample queries
- [ ] Test metric calculation accuracy
- [ ] Test CLI evaluation command
- [ ] Test with/without ground truth
- [ ] Create baseline test set for v0.3.x tracking

#### Files to Create

- `src/evaluation/ragas_evaluator.py` (~200 lines)
- `src/cli/commands/evaluate.py` (~150 lines)
- `tests/evaluation/test_ragas_evaluator.py` (~100 lines)
- `tests/cli/test_evaluate.py` (~80 lines)
- `data/test_sets/baseline_v0.3.json` (baseline test questions)

#### Dependencies

```toml
ragas = "^0.1.0"              # Apache 2.0 - Evaluation framework
datasets = "^2.14.0"           # Apache 2.0 - Required by ragas
```

#### Acceptance Criteria

- ✅ RAGAS framework integrated
- ✅ CLI command `ragged evaluate ragas test_set.json` works
- ✅ Baseline scores established for v0.2 (current state)
- ✅ Scores saved for v0.3.x comparison

---

### FEAT-010: Answer Confidence Scores (8h)

**Priority:** High
**Dependencies:** None

#### Scope

Implement confidence scoring for generated answers. Helps users understand answer reliability.

**Confidence Factors:**
1. Retrieval score (chunk similarity)
2. LLM generation confidence
3. Citation coverage (how much of answer is cited)

#### Implementation

```python
# src/generation/confidence.py (NEW FILE)
"""Answer confidence scoring."""
from typing import List
from dataclasses import dataclass

@dataclass
class ConfidenceScore:
    """Confidence breakdown for an answer."""
    retrieval_score: float      # 0-1: How relevant are retrieved chunks?
    generation_score: float      # 0-1: LLM confidence in answer
    citation_coverage: float     # 0-1: How much is cited?
    overall_confidence: float    # 0-1: Weighted average

    def to_str(self) -> str:
        """Human-readable confidence level."""
        if self.overall_confidence >= 0.9:
            return "Very High"
        elif self.overall_confidence >= 0.75:
            return "High"
        elif self.overall_confidence >= 0.6:
            return "Medium"
        elif self.overall_confidence >= 0.4:
            return "Low"
        else:
            return "Very Low"

class ConfidenceCalculator:
    """Calculate answer confidence scores."""

    def __init__(
        self,
        retrieval_weight: float = 0.4,
        generation_weight: float = 0.3,
        citation_weight: float = 0.3
    ):
        """
        Initialise calculator with weights.

        Args:
            retrieval_weight: Weight for retrieval score
            generation_weight: Weight for LLM confidence
            citation_weight: Weight for citation coverage
        """
        self.retrieval_weight = retrieval_weight
        self.generation_weight = generation_weight
        self.citation_weight = citation_weight

    def calculate(
        self,
        retrieved_chunks: List,
        answer: str,
        citations: List[str]
    ) -> ConfidenceScore:
        """
        Calculate confidence for an answer.

        Args:
            retrieved_chunks: Retrieved chunks with scores
            answer: Generated answer
            citations: Extracted citations

        Returns:
            ConfidenceScore with breakdown
        """
        # Retrieval score: Average of top-k chunk scores
        if retrieved_chunks:
            retrieval_score = sum(
                c.score for c in retrieved_chunks
            ) / len(retrieved_chunks)
        else:
            retrieval_score = 0.0

        # Generation score: Placeholder (will use LLM logprobs in future)
        # For now, use heuristics: length, clarity, etc.
        generation_score = self._estimate_generation_quality(answer)

        # Citation coverage: Percentage of answer with citations
        citation_coverage = self._calculate_citation_coverage(
            answer, citations
        )

        # Overall confidence: Weighted average
        overall = (
            self.retrieval_weight * retrieval_score +
            self.generation_weight * generation_score +
            self.citation_weight * citation_coverage
        )

        return ConfidenceScore(
            retrieval_score=retrieval_score,
            generation_score=generation_score,
            citation_coverage=citation_coverage,
            overall_confidence=overall
        )

    def _estimate_generation_quality(self, answer: str) -> float:
        """
        Estimate generation quality (placeholder).

        Future: Use LLM logprobs when available.
        Current: Basic heuristics.
        """
        # Check for common quality signals
        has_structure = any(marker in answer for marker in ['1.', '2.', '-', '*'])
        reasonable_length = 50 <= len(answer) <= 2000
        not_too_repetitive = len(set(answer.split())) / len(answer.split()) > 0.5

        score = 0.5  # Neutral baseline
        if has_structure:
            score += 0.15
        if reasonable_length:
            score += 0.15
        if not_too_repetitive:
            score += 0.2

        return min(score, 1.0)

    def _calculate_citation_coverage(
        self,
        answer: str,
        citations: List[str]
    ) -> float:
        """Calculate what percentage of answer is cited."""
        if not citations:
            return 0.0

        # Rough estimate: Count citation markers
        citation_count = answer.count('[Source:')
        words_per_citation = 50  # Assume each citation covers ~50 words

        total_words = len(answer.split())
        cited_words = min(citation_count * words_per_citation, total_words)

        return cited_words / total_words if total_words > 0 else 0.0
```

**Integration:**
```python
# src/cli/commands/query.py - enhance query command
@click.option('--show-confidence', is_flag=True, help='Show answer confidence')
def query(query_text, show_confidence, ...):
    """Generate answer with optional confidence display."""
    # ... existing retrieval and generation ...

    if show_confidence:
        from src.generation.confidence import ConfidenceCalculator

        calculator = ConfidenceCalculator()
        confidence = calculator.calculate(
            retrieved_chunks=chunks,
            answer=answer,
            citations=citations
        )

        click.echo(f"\nConfidence: {confidence.to_str()} ({confidence.overall_confidence:.2f})")
        click.echo(f"  Retrieval:  {confidence.retrieval_score:.2f}")
        click.echo(f"  Generation: {confidence.generation_score:.2f}")
        click.echo(f"  Citations:  {confidence.citation_coverage:.2f}")
```

#### Testing Requirements

- [ ] Test confidence calculation with various answer qualities
- [ ] Test CLI `--show-confidence` flag
- [ ] Test edge cases (no chunks, no citations)
- [ ] Validate confidence correlates with actual quality

#### Files to Create

- `src/generation/confidence.py` (~180 lines)
- `tests/generation/test_confidence.py` (~100 lines)

#### Files to Modify

- `src/cli/commands/query.py` (add `--show-confidence` option)

#### Acceptance Criteria

- ✅ Confidence scores calculated for all answers
- ✅ CLI displays confidence when requested
- ✅ Confidence breakdown available
- ✅ Scores validated against manual assessment

---

## Implementation Phases

### Phase 1: Design & Setup (4h)

**Session 1: Architecture Design (2h)**
- Review RAGAS framework documentation
- Design integration architecture
- Plan module structure
- Define interfaces and data models

**Session 2: Development Environment (2h)**
- Add `ragas` and `datasets` dependencies
- Set up test data structures
- Create baseline test set (20+ queries)
- Configure development environment

**Deliverables:**
- Architecture decision documented
- Dependencies installed
- Test set prepared
- Development environment ready

### Phase 2: Core Implementation (16h)

**Session 3: RAGAS Evaluator Module (4h)**
- Create `src/evaluation/ragas_evaluator.py`
- Implement `RAGASEvaluator` class
- Implement `EvaluationResult` dataclass
- Add metric integration (precision, recall, faithfulness, relevancy)

**Session 4: Confidence Scoring Module (4h)**
- Create `src/generation/confidence.py`
- Implement `ConfidenceCalculator` class
- Implement `ConfidenceScore` dataclass
- Add retrieval/generation/citation scoring logic

**Session 5: CLI Integration - Evaluate Command (4h)**
- Create `src/cli/commands/evaluate.py`
- Implement `ragged evaluate ragas` command
- Add test set loading and validation
- Add result formatting (table/JSON)

**Session 6: CLI Integration - Confidence Display (4h)**
- Modify `src/cli/commands/query.py`
- Add `--show-confidence` flag
- Integrate confidence calculation
- Add confidence display formatting

**Deliverables:**
- 2 new modules fully implemented
- 2 CLI commands functional
- Code reviewed and formatted

### Phase 3: Testing & Validation (6h)

**Session 7: Unit Tests (3h)**
- Create `tests/evaluation/test_ragas_evaluator.py`
- Create `tests/generation/test_confidence.py`
- Create `tests/cli/test_evaluate.py`
- Achieve 100% coverage for new code

**Session 8: Integration Testing & Baseline (3h)**
- Run end-to-end evaluation pipeline
- Establish v0.2 baseline scores
- Validate confidence scoring accuracy
- Test edge cases (no chunks, no citations)

**Deliverables:**
- All tests passing
- Baseline metrics documented
- Quality validated

### Phase 4: Documentation & Release (4h)

**Session 9: Documentation (2h)**
- Use documentation-architect agent for planning
- Document evaluation usage in user guides
- Document confidence scoring feature
- Add examples and use cases

**Session 10: Audit & Commit (2h)**
- Run documentation-auditor agent
- Fix any issues identified
- Use git-documentation-committer agent
- Tag v0.3.1 release

**Deliverables:**
- Complete documentation
- Quality audit passed
- Release tagged

---

## Technical Architecture

### Module Structure

```
src/
├── evaluation/
│   ├── __init__.py
│   └── ragas_evaluator.py          # RAGAS integration (200 lines)
│       ├── class RAGASEvaluator
│       ├── class EvaluationResult
│       └── evaluate() methods
│
├── generation/
│   ├── confidence.py                # Confidence scoring (180 lines)
│   │   ├── class ConfidenceCalculator
│   │   ├── class ConfidenceScore
│   │   └── calculate() methods
│   └── ...
│
└── cli/commands/
    ├── evaluate.py                  # Evaluation commands (150 lines)
    │   └── ragged evaluate ragas
    └── query.py                     # Enhanced with --show-confidence

tests/
├── evaluation/
│   └── test_ragas_evaluator.py      # RAGAS tests (100 lines)
├── generation/
│   └── test_confidence.py           # Confidence tests (100 lines)
└── cli/
    └── test_evaluate.py             # CLI tests (80 lines)
```

### Data Flow

**RAGAS Evaluation Flow:**
```
Test Set (JSON)
    ↓
Load questions + ground truths
    ↓
For each question:
    ↓
Retrieve(question) → chunks
    ↓
Generate(question, chunks) → answer
    ↓
RAGAS.evaluate(question, answer, chunks, ground_truth)
    ↓
EvaluationResult
    ↓
Display (table/JSON)
```

**Confidence Scoring Flow:**
```
Query → Retrieval → chunks (with scores)
    ↓
Generate → answer + citations
    ↓
ConfidenceCalculator.calculate()
    ├─ retrieval_score (avg chunk similarity)
    ├─ generation_score (heuristics)
    └─ citation_coverage (% cited)
    ↓
ConfidenceScore (overall + breakdown)
    ↓
Display with answer
```

### API Interfaces

**RAGASEvaluator Interface:**
```python
class RAGASEvaluator:
    def evaluate(
        questions: List[str],
        answers: List[str],
        contexts: List[List[str]],
        ground_truths: Optional[List[str]]
    ) -> EvaluationResult

    def evaluate_single(
        question: str,
        answer: str,
        contexts: List[str],
        ground_truth: Optional[str]
    ) -> EvaluationResult
```

**ConfidenceCalculator Interface:**
```python
class ConfidenceCalculator:
    def calculate(
        retrieved_chunks: List[Chunk],
        answer: str,
        citations: List[str]
    ) -> ConfidenceScore
```

### Integration Points

1. **CLI → Evaluation Module**
   - `ragged evaluate ragas` command calls `RAGASEvaluator`
   - Test set loaded from JSON file
   - Results formatted for display

2. **CLI → Confidence Module**
   - `ragged query --show-confidence` calls `ConfidenceCalculator`
   - Integrates with existing query pipeline
   - Optional display (flag-controlled)

3. **Evaluation → Retrieval → Generation**
   - RAGAS evaluation uses existing `Retriever` and `OllamaClient`
   - No modifications to core components needed
   - Clean separation of concerns

---

## Risk Analysis & Mitigation

### Technical Risks

**Risk 1: RAGAS Framework Compatibility**
- **Impact:** High - Core feature dependency
- **Probability:** Low - RAGAS is mature
- **Mitigation:**
  - Test RAGAS integration early (Session 1)
  - Have fallback to custom metrics if needed
  - Pin RAGAS version to avoid breaking changes
- **Detection:** Integration tests in Session 7

**Risk 2: Confidence Scoring Accuracy**
- **Impact:** Medium - Affects user trust
- **Probability:** Medium - Heuristics are imperfect
- **Mitigation:**
  - Start with conservative estimates
  - Validate against manual assessment
  - Clearly document confidence methodology
  - Plan to improve with LLM logprobs in future
- **Detection:** Manual validation in Session 8

**Risk 3: Test Set Quality**
- **Impact:** High - Poor baseline invalidates comparisons
- **Probability:** Medium - Creating good test sets is hard
- **Mitigation:**
  - Diverse test set covering various query types
  - Include edge cases (ambiguous, multi-hop)
  - Iterate on test set based on results
  - Document test set creation methodology
- **Detection:** Review baseline scores in Session 8

### Performance Risks

**Risk 4: RAGAS Evaluation Latency**
- **Impact:** Low - Only used for testing, not user queries
- **Probability:** Medium - RAGAS uses LLM for evaluation
- **Mitigation:**
  - Run evaluations offline
  - Use smaller LLM for RAGAS if needed
  - Cache evaluation results
  - Not performance-critical for user experience
- **Detection:** Measure evaluation time in Session 8

### UX Risks

**Risk 5: Confidence Score Interpretation**
- **Impact:** Medium - Users may misunderstand scores
- **Probability:** Medium - Numeric scores can be ambiguous
- **Mitigation:**
  - Provide verbal labels ("High", "Medium", "Low")
  - Show breakdown (retrieval/generation/citations)
  - Document what confidence means
  - Make `--show-confidence` optional (not default)
- **Detection:** User feedback after release

---

## Quality Gates

### Functional Requirements

- [ ] RAGAS framework integrated successfully
- [ ] All 4 RAGAS metrics calculated (precision, recall, faithfulness, relevancy)
- [ ] CLI command `ragged evaluate ragas test_set.json` works
- [ ] Test set format validated and documented
- [ ] Confidence scoring implemented for all queries
- [ ] CLI flag `--show-confidence` works correctly
- [ ] Confidence breakdown displayed (retrieval/generation/citations)
- [ ] Baseline v0.2 scores established and documented

### Performance Requirements

- [ ] RAGAS evaluation completes within reasonable time (<5 min for 20 queries)
- [ ] Confidence calculation adds <100ms overhead to queries
- [ ] Memory usage acceptable (no leaks, <500MB for evaluation)

### Code Quality Requirements

- [ ] 100% test coverage for new code
- [ ] All unit tests passing
- [ ] All integration tests passing
- [ ] Code follows project style guide
- [ ] Type hints complete
- [ ] Docstrings complete (British English)
- [ ] No linting errors

### Documentation Requirements

- [ ] User guide updated with evaluation feature
- [ ] Confidence scoring documented
- [ ] API documentation complete
- [ ] Examples provided for both features
- [ ] Baseline methodology documented
- [ ] documentation-auditor check passed
- [ ] British English compliance verified

### Baseline Score Requirements

- [ ] Context Precision: > 0.60 (v0.2 baseline)
- [ ] Context Recall: > 0.60 (v0.2 baseline)
- [ ] Faithfulness: > 0.70 (v0.2 baseline)
- [ ] Answer Relevancy: > 0.65 (v0.2 baseline)
- [ ] Overall Score: > 0.65 (v0.2 baseline)
- [ ] Confidence scores correlate with manual quality assessment (>0.7 correlation)

---

## Execution Checklist

### Pre-Implementation Setup

- [ ] Create feature branch: `git checkout -b feature/v0.3.1-foundation-metrics`
- [ ] Review RAGAS documentation: https://docs.ragas.io/
- [ ] Review confidence scoring best practices
- [ ] Set up time tracking for this version
- [ ] Prepare development environment

### Session-by-Session Tasks

**Session 1: Architecture Design**
- [ ] Read RAGAS framework docs
- [ ] Design `RAGASEvaluator` class structure
- [ ] Design `ConfidenceCalculator` class structure
- [ ] Document architecture decisions
- [ ] Get feedback on design (optional: architecture-advisor agent)

**Session 2: Development Environment**
- [ ] Add `ragas = "^0.1.0"` to `pyproject.toml`
- [ ] Add `datasets = "^2.14.0"` to `pyproject.toml`
- [ ] Run `poetry install`
- [ ] Verify RAGAS import works
- [ ] Create `data/test_sets/baseline_v0.3.json` with 20+ queries
- [ ] Validate test set format

**Session 3: RAGAS Evaluator Module**
- [ ] Create `src/evaluation/__init__.py`
- [ ] Create `src/evaluation/ragas_evaluator.py`
- [ ] Implement `EvaluationResult` dataclass
- [ ] Implement `RAGASEvaluator.__init__()`
- [ ] Implement `RAGASEvaluator.evaluate()`
- [ ] Implement `RAGASEvaluator.evaluate_single()`
- [ ] Add type hints and docstrings
- [ ] Test manually with sample data

**Session 4: Confidence Scoring Module**
- [ ] Create `src/generation/confidence.py`
- [ ] Implement `ConfidenceScore` dataclass
- [ ] Implement `ConfidenceCalculator.__init__()`
- [ ] Implement `ConfidenceCalculator.calculate()`
- [ ] Implement `_estimate_generation_quality()`
- [ ] Implement `_calculate_citation_coverage()`
- [ ] Add type hints and docstrings
- [ ] Test manually with sample queries

**Session 5: CLI Integration - Evaluate Command**
- [ ] Create `src/cli/commands/evaluate.py`
- [ ] Implement `evaluate()` group command
- [ ] Implement `ragas()` command
- [ ] Add test set loading logic
- [ ] Add result formatting (table format)
- [ ] Add result formatting (JSON format)
- [ ] Test with sample test set
- [ ] Verify error handling

**Session 6: CLI Integration - Confidence Display**
- [ ] Modify `src/cli/commands/query.py`
- [ ] Add `--show-confidence` flag to `query()` command
- [ ] Import `ConfidenceCalculator`
- [ ] Add confidence calculation after generation
- [ ] Add confidence display formatting
- [ ] Test with various queries
- [ ] Verify flag works correctly

**Session 7: Unit Tests**
- [ ] Create `tests/evaluation/test_ragas_evaluator.py`
- [ ] Test `RAGASEvaluator.evaluate()` with sample data
- [ ] Test `RAGASEvaluator.evaluate_single()`
- [ ] Test edge cases (empty data, None values)
- [ ] Create `tests/generation/test_confidence.py`
- [ ] Test `ConfidenceCalculator.calculate()`
- [ ] Test confidence scoring edge cases
- [ ] Create `tests/cli/test_evaluate.py`
- [ ] Test CLI evaluate command
- [ ] Run `pytest --cov` and verify 100% coverage for new code

**Session 8: Integration Testing & Baseline**
- [ ] Run `ragged evaluate ragas data/test_sets/baseline_v0.3.json`
- [ ] Record v0.2 baseline scores in documentation
- [ ] Test 10+ queries with `--show-confidence`
- [ ] Manually assess answer quality
- [ ] Calculate correlation between confidence and quality
- [ ] Validate correlation > 0.7
- [ ] Test edge cases (no retrieval results, empty answers)
- [ ] Fix any issues discovered

**Session 9: Documentation**
- [ ] Use documentation-architect agent to plan structure
- [ ] Update `docs/guides/cli/intermediate.md` with evaluation feature
- [ ] Add confidence scoring to user guide
- [ ] Create examples for both features
- [ ] Document test set format
- [ ] Document baseline methodology
- [ ] Add to CHANGELOG.md

**Session 10: Audit & Commit**
- [ ] Run documentation-auditor agent
- [ ] Fix British English issues
- [ ] Fix cross-reference issues
- [ ] Fix any SSOT violations
- [ ] Run `/check-british` command
- [ ] Use git-documentation-committer agent
- [ ] Verify commit message follows conventions
- [ ] Push to GitHub
- [ ] Create tag: `git tag v0.3.1`
- [ ] Push tag: `git push origin v0.3.1`

### Post-Implementation

- [ ] Update `docs/development/implementation/version/v0.3/v0.3.1.md` with results
- [ ] Record actual hours spent in time log
- [ ] Compare actual vs estimated effort
- [ ] Document lessons learned
- [ ] Update progress in v0.3/README.md
- [ ] Prepare for v0.3.2

---

## Agent Workflow (6h)

### 1. documentation-architect (2h)

**Task:** Plan documentation structure for evaluation features

**Deliverables:**
- Decide where evaluation docs belong
- Plan user guide structure
- Determine cross-references

### 2. documentation-auditor (2h)

**Task:** Comprehensive review before commit

**Checks:**
- SSOT compliance
- British English
- Directory coverage
- Cross-references

### 3. git-documentation-committer (2h)

**Task:** Commit with quality checks

**Workflow:**
1. Run documentation audit
2. Fix any issues
3. Create conventional commit
4. AI attribution

---

## Testing Strategy

**Unit Tests:**
- RAGAS evaluator logic
- Confidence calculator
- CLI commands

**Integration Tests:**
- End-to-end evaluation pipeline
- Confidence scoring in query flow

**Baseline Establishment:**
- Run RAGAS on 20+ test queries
- Record v0.2 baseline scores
- Save for v0.3.x comparison

---

## Deliverables

1. **RAGAS Framework Integrated**
   - CLI command: `ragged evaluate ragas test_set.json`
   - Automated quality metrics
   - Baseline scores established

2. **Confidence Scoring**
   - Per-answer confidence calculation
   - CLI option: `ragged query --show-confidence "question"`
   - Confidence breakdown (retrieval/generation/citations)

3. **Baseline Metrics**
   - v0.2 RAGAS scores documented
   - Test set created for ongoing validation
   - Comparison framework ready for v0.3.x

---

## Success Criteria

- ✅ RAGAS evaluation runs successfully on test set
- ✅ Baseline scores: Context Precision > 0.6, Overall > 0.65
- ✅ Confidence scores correlate with manual quality assessment
- ✅ Documentation complete and audited
- ✅ All tests passing (target: 100% for new code)

---

## Related Documentation

- [v0.3.0 Roadmap](./README.md) - Overview
- [Evaluation & Quality Features](./features/evaluation-quality.md) - Detailed specs
- [v0.3.10 - Performance & Quality Tools](./v0.3.10.md) - Related metrics features

---


**License:** GPL-3.0

**Status:** Planned
