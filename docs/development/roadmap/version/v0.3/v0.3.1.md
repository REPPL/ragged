# v0.3.1 - Foundation & Metrics

**Category:** Quality & Evaluation Infrastructure

**Estimated Time:** 30 hours

**Status:** Planned

---

## Overview

Establish objective quality metrics BEFORE implementing any improvements. This enables data-driven development by providing baseline scores for comparison.

**Why First:** Measuring quality improvement requires baselines. Without metrics, we can't validate that v0.3.x improvements actually work.

---

## Features

### FEAT-009: RAGAS Evaluation Framework (16h)

**Priority:** Critical
**Dependencies:** None

#### Scope

Integrate RAGAS (Retrieval-Augmented Generation Assessment) framework for automated quality evaluation.

**Metrics to Track:**
1. **Context Precision:** Are retrieved chunks relevant?
2. **Context Recall:** Did we retrieve all relevant chunks?
3. **Faithfulness:** Does answer match context?
4. **Answer Relevance:** Does answer address the question?

#### Implementation

```python
# src/evaluation/ragas_evaluator.py (NEW FILE)
"""RAGAS-based quality evaluation."""
from typing import List, Dict
from dataclasses import dataclass
from ragas import evaluate
from ragas.metrics import (
    context_precision,
    context_recall,
    faithfulness,
    answer_relevancy
)

@dataclass
class EvaluationResult:
    """Results of RAGAS evaluation."""
    context_precision: float
    context_recall: float
    faithfulness: float
    answer_relevancy: float
    overall_score: float

    def to_dict(self) -> Dict:
        """Convert to dictionary."""
        return {
            'context_precision': self.context_precision,
            'context_recall': self.context_recall,
            'faithfulness': self.faithfulness,
            'answer_relevancy': self.answer_relevancy,
            'overall_score': self.overall_score
        }

class RAGASEvaluator:
    """
    Evaluate RAG pipeline quality using RAGAS framework.

    Provides automated, objective quality metrics.
    """

    def __init__(self):
        """Initialise evaluator with default metrics."""
        self.metrics = [
            context_precision,
            context_recall,
            faithfulness,
            answer_relevancy
        ]

    def evaluate(
        self,
        questions: List[str],
        answers: List[str],
        contexts: List[List[str]],
        ground_truths: List[str] = None
    ) -> EvaluationResult:
        """
        Evaluate RAG pipeline performance.

        Args:
            questions: User queries
            answers: Generated answers
            contexts: Retrieved chunks for each query
            ground_truths: Optional reference answers

        Returns:
            EvaluationResult with metric scores
        """
        # Prepare dataset
        dataset = {
            'question': questions,
            'answer': answers,
            'contexts': contexts
        }

        if ground_truths:
            dataset['ground_truth'] = ground_truths

        # Run evaluation
        result = evaluate(
            dataset,
            metrics=self.metrics
        )

        # Calculate overall score
        scores = [
            result['context_precision'],
            result['context_recall'],
            result['faithfulness'],
            result['answer_relevancy']
        ]
        overall = sum(scores) / len(scores)

        return EvaluationResult(
            context_precision=result['context_precision'],
            context_recall=result['context_recall'],
            faithfulness=result['faithfulness'],
            answer_relevancy=result['answer_relevancy'],
            overall_score=overall
        )

    def evaluate_single(
        self,
        question: str,
        answer: str,
        contexts: List[str],
        ground_truth: str = None
    ) -> EvaluationResult:
        """Evaluate a single query-answer pair."""
        return self.evaluate(
            questions=[question],
            answers=[answer],
            contexts=[contexts],
            ground_truths=[ground_truth] if ground_truth else None
        )
```

**CLI Integration:**
```python
# src/cli/commands/evaluate.py (NEW FILE)
"""Quality evaluation commands."""
import click
from src.evaluation.ragas_evaluator import RAGASEvaluator

@click.group()
def evaluate():
    """Evaluate RAG pipeline quality."""
    pass

@evaluate.command()
@click.argument('test_set', type=click.Path(exists=True))
@click.option('--format', type=click.Choice(['table', 'json']), default='table')
def ragas(test_set, format):
    """
    Evaluate using RAGAS framework.

    TEST_SET: Path to JSON file with test questions/answers

    Expected format:
    [
        {
            "question": "What is RAG?",
            "ground_truth": "Retrieval-Augmented Generation..."
        }
    ]
    """
    import json
    from src.storage.vector_store import VectorStore
    from src.retrieval.retriever import Retriever
    from src.generation.ollama_client import OllamaClient

    # Load test set
    with open(test_set) as f:
        tests = json.load(f)

    # Initialize components
    store = VectorStore()
    retriever = Retriever(vector_store=store)
    llm = OllamaClient()
    evaluator = RAGASEvaluator()

    # Run queries
    questions = [t['question'] for t in tests]
    ground_truths = [t.get('ground_truth') for t in tests]

    answers = []
    contexts = []

    for question in questions:
        # Retrieve
        chunks = retriever.retrieve(question, top_k=5)
        context = [c.content for c in chunks]
        contexts.append(context)

        # Generate
        answer = llm.generate(
            query=question,
            context='\n\n'.join(context)
        )
        answers.append(answer)

    # Evaluate
    result = evaluator.evaluate(
        questions=questions,
        answers=answers,
        contexts=contexts,
        ground_truths=ground_truths if any(ground_truths) else None
    )

    # Display results
    if format == 'table':
        click.echo("\nRAGAS Evaluation Results:")
        click.echo(f"  Context Precision: {result.context_precision:.3f}")
        click.echo(f"  Context Recall:    {result.context_recall:.3f}")
        click.echo(f"  Faithfulness:      {result.faithfulness:.3f}")
        click.echo(f"  Answer Relevancy:  {result.answer_relevancy:.3f}")
        click.echo(f"  Overall Score:     {result.overall_score:.3f}")
    else:
        click.echo(json.dumps(result.to_dict(), indent=2))
```

#### Testing Requirements

- [ ] Test RAGAS integration with sample queries
- [ ] Test metric calculation accuracy
- [ ] Test CLI evaluation command
- [ ] Test with/without ground truth
- [ ] Create baseline test set for v0.3.x tracking

#### Files to Create

- `src/evaluation/ragas_evaluator.py` (~200 lines)
- `src/cli/commands/evaluate.py` (~150 lines)
- `tests/evaluation/test_ragas_evaluator.py` (~100 lines)
- `tests/cli/test_evaluate.py` (~80 lines)
- `data/test_sets/baseline_v0.3.json` (baseline test questions)

#### Dependencies

```toml
ragas = "^0.1.0"              # Apache 2.0 - Evaluation framework
datasets = "^2.14.0"           # Apache 2.0 - Required by ragas
```

#### Acceptance Criteria

- ✅ RAGAS framework integrated
- ✅ CLI command `ragged evaluate ragas test_set.json` works
- ✅ Baseline scores established for v0.2 (current state)
- ✅ Scores saved for v0.3.x comparison

---

### FEAT-010: Answer Confidence Scores (8h)

**Priority:** High
**Dependencies:** None

#### Scope

Implement confidence scoring for generated answers. Helps users understand answer reliability.

**Confidence Factors:**
1. Retrieval score (chunk similarity)
2. LLM generation confidence
3. Citation coverage (how much of answer is cited)

#### Implementation

```python
# src/generation/confidence.py (NEW FILE)
"""Answer confidence scoring."""
from typing import List
from dataclasses import dataclass

@dataclass
class ConfidenceScore:
    """Confidence breakdown for an answer."""
    retrieval_score: float      # 0-1: How relevant are retrieved chunks?
    generation_score: float      # 0-1: LLM confidence in answer
    citation_coverage: float     # 0-1: How much is cited?
    overall_confidence: float    # 0-1: Weighted average

    def to_str(self) -> str:
        """Human-readable confidence level."""
        if self.overall_confidence >= 0.9:
            return "Very High"
        elif self.overall_confidence >= 0.75:
            return "High"
        elif self.overall_confidence >= 0.6:
            return "Medium"
        elif self.overall_confidence >= 0.4:
            return "Low"
        else:
            return "Very Low"

class ConfidenceCalculator:
    """Calculate answer confidence scores."""

    def __init__(
        self,
        retrieval_weight: float = 0.4,
        generation_weight: float = 0.3,
        citation_weight: float = 0.3
    ):
        """
        Initialise calculator with weights.

        Args:
            retrieval_weight: Weight for retrieval score
            generation_weight: Weight for LLM confidence
            citation_weight: Weight for citation coverage
        """
        self.retrieval_weight = retrieval_weight
        self.generation_weight = generation_weight
        self.citation_weight = citation_weight

    def calculate(
        self,
        retrieved_chunks: List,
        answer: str,
        citations: List[str]
    ) -> ConfidenceScore:
        """
        Calculate confidence for an answer.

        Args:
            retrieved_chunks: Retrieved chunks with scores
            answer: Generated answer
            citations: Extracted citations

        Returns:
            ConfidenceScore with breakdown
        """
        # Retrieval score: Average of top-k chunk scores
        if retrieved_chunks:
            retrieval_score = sum(
                c.score for c in retrieved_chunks
            ) / len(retrieved_chunks)
        else:
            retrieval_score = 0.0

        # Generation score: Placeholder (will use LLM logprobs in future)
        # For now, use heuristics: length, clarity, etc.
        generation_score = self._estimate_generation_quality(answer)

        # Citation coverage: Percentage of answer with citations
        citation_coverage = self._calculate_citation_coverage(
            answer, citations
        )

        # Overall confidence: Weighted average
        overall = (
            self.retrieval_weight * retrieval_score +
            self.generation_weight * generation_score +
            self.citation_weight * citation_coverage
        )

        return ConfidenceScore(
            retrieval_score=retrieval_score,
            generation_score=generation_score,
            citation_coverage=citation_coverage,
            overall_confidence=overall
        )

    def _estimate_generation_quality(self, answer: str) -> float:
        """
        Estimate generation quality (placeholder).

        Future: Use LLM logprobs when available.
        Current: Basic heuristics.
        """
        # Check for common quality signals
        has_structure = any(marker in answer for marker in ['1.', '2.', '-', '*'])
        reasonable_length = 50 <= len(answer) <= 2000
        not_too_repetitive = len(set(answer.split())) / len(answer.split()) > 0.5

        score = 0.5  # Neutral baseline
        if has_structure:
            score += 0.15
        if reasonable_length:
            score += 0.15
        if not_too_repetitive:
            score += 0.2

        return min(score, 1.0)

    def _calculate_citation_coverage(
        self,
        answer: str,
        citations: List[str]
    ) -> float:
        """Calculate what percentage of answer is cited."""
        if not citations:
            return 0.0

        # Rough estimate: Count citation markers
        citation_count = answer.count('[Source:')
        words_per_citation = 50  # Assume each citation covers ~50 words

        total_words = len(answer.split())
        cited_words = min(citation_count * words_per_citation, total_words)

        return cited_words / total_words if total_words > 0 else 0.0
```

**Integration:**
```python
# src/cli/commands/query.py - enhance query command
@click.option('--show-confidence', is_flag=True, help='Show answer confidence')
def query(query_text, show_confidence, ...):
    """Generate answer with optional confidence display."""
    # ... existing retrieval and generation ...

    if show_confidence:
        from src.generation.confidence import ConfidenceCalculator

        calculator = ConfidenceCalculator()
        confidence = calculator.calculate(
            retrieved_chunks=chunks,
            answer=answer,
            citations=citations
        )

        click.echo(f"\nConfidence: {confidence.to_str()} ({confidence.overall_confidence:.2f})")
        click.echo(f"  Retrieval:  {confidence.retrieval_score:.2f}")
        click.echo(f"  Generation: {confidence.generation_score:.2f}")
        click.echo(f"  Citations:  {confidence.citation_coverage:.2f}")
```

#### Testing Requirements

- [ ] Test confidence calculation with various answer qualities
- [ ] Test CLI `--show-confidence` flag
- [ ] Test edge cases (no chunks, no citations)
- [ ] Validate confidence correlates with actual quality

#### Files to Create

- `src/generation/confidence.py` (~180 lines)
- `tests/generation/test_confidence.py` (~100 lines)

#### Files to Modify

- `src/cli/commands/query.py` (add `--show-confidence` option)

#### Acceptance Criteria

- ✅ Confidence scores calculated for all answers
- ✅ CLI displays confidence when requested
- ✅ Confidence breakdown available
- ✅ Scores validated against manual assessment

---

## Agent Workflow (6h)

### 1. documentation-architect (2h)

**Task:** Plan documentation structure for evaluation features

**Deliverables:**
- Decide where evaluation docs belong
- Plan user guide structure
- Determine cross-references

### 2. documentation-auditor (2h)

**Task:** Comprehensive review before commit

**Checks:**
- SSOT compliance
- British English
- Directory coverage
- Cross-references

### 3. git-documentation-committer (2h)

**Task:** Commit with quality checks

**Workflow:**
1. Run documentation audit
2. Fix any issues
3. Create conventional commit
4. AI attribution

---

## Testing Strategy

**Unit Tests:**
- RAGAS evaluator logic
- Confidence calculator
- CLI commands

**Integration Tests:**
- End-to-end evaluation pipeline
- Confidence scoring in query flow

**Baseline Establishment:**
- Run RAGAS on 20+ test queries
- Record v0.2 baseline scores
- Save for v0.3.x comparison

---

## Deliverables

1. **RAGAS Framework Integrated**
   - CLI command: `ragged evaluate ragas test_set.json`
   - Automated quality metrics
   - Baseline scores established

2. **Confidence Scoring**
   - Per-answer confidence calculation
   - CLI option: `ragged query --show-confidence "question"`
   - Confidence breakdown (retrieval/generation/citations)

3. **Baseline Metrics**
   - v0.2 RAGAS scores documented
   - Test set created for ongoing validation
   - Comparison framework ready for v0.3.x

---

## Success Criteria

- ✅ RAGAS evaluation runs successfully on test set
- ✅ Baseline scores: Context Precision > 0.6, Overall > 0.65
- ✅ Confidence scores correlate with manual quality assessment
- ✅ Documentation complete and audited
- ✅ All tests passing (target: 100% for new code)

---

## Related Documentation

- [v0.3.0 Roadmap](./README.md) - Overview
- [Evaluation & Quality Features](./features/evaluation-quality.md) - Detailed specs
- [v0.3.10 - Performance & Quality Tools](./v0.3.10.md) - Related metrics features

---

**Maintained By:** ragged development team

**License:** GPL-3.0

**Status:** Planned
